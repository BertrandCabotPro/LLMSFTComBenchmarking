[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Config[Trainer(
  accelerator='gpu',
  strategy=<Config[MegatronStrategy(
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    virtual_pipeline_model_parallel_size=None,
    context_parallel_size=1,
    sequence_parallel=False,
    pipeline_dtype=torch.bfloat16,
    ckpt_load_strictness='log_all',
    gradient_as_bucket_view=True)]>,
  devices=4,
  num_nodes=16,
  callbacks=[<Config[TimingCallback()]>],
  max_steps=1000,
  limit_val_batches=None,
  limit_test_batches=None,
  val_check_interval=30,
  log_every_n_steps=1,
  accumulate_grad_batches=1,
  use_distributed_sampler=False,
  plugins=<Config[MegatronMixedPrecision(
    precision='bf16-mixed',
    params_dtype=torch.bfloat16,
    pipeline_dtype=torch.bfloat16,
    autocast_enabled=False,
    grad_reduce_in_fp32=True)]>)]>
<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>
<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>
Mon Nov 10 01:28:57 CET 2025
