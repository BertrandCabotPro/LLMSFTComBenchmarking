[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x146645cab2c0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1490e7e3eb40>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1495fe008cb0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14eff2dfda60>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x15377d6ae0f0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x145138a26240>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14e31f43bc20>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x146ce9da6c30>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14921f5a6e70>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x145b5cd41ca0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1538d7212960>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x150c76143b90>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x149beef82360>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1503c17f1c40>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x150fe4c2f5f0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14c6cea3ed80>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14c52fe44440>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x154f38407050>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14c0d643bb00>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14948d98eba0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14e6eb293170>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14ea643d73e0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x148add89f980>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14efaab850d0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14f27891bdd0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14d3a784c0b0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
Mon Nov 10 02:10:51 CET 2025
