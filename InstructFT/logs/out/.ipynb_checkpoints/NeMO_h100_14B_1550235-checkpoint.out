[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14e8a2a6da00>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x146719077b30>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14bd2b0171d0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x148013bf4890>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1494277c93a0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14be78cac470>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14b7454ec0b0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x147550aca750>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14eba5fe6f30>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1498954eea80>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x147123f0b5f0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x153f053b42c0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x149cf7fd2de0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14c832bcebd0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14a114cd3290>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1479f16f2990>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1494e8c3b470>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x152591056ed0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1479624288f0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1539499c7d10>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x153e245e4500>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1456e63c85c0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<Config[SquadDataModule(seq_length=2048, micro_batch_size=1, global_batch_size=128)]>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=1,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[<Config[TimingCallback()]>],
    max_steps=1000,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=1,
    accumulate_grad_batches=1,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=<Config[ModelCheckpoint(
      save_last='link',
      save_top_k=2,
      every_n_train_steps=50,
      filename='{model_name}--{val_loss:.2f}-{step}-{consumed_samples}')]>,
    tensorboard=<Config[TensorBoardLogger(save_dir='tb_logs', name='Qwen/Qwen2.5-14B-Instruct')]>,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
Mon Nov 10 03:15:32 CET 2025
