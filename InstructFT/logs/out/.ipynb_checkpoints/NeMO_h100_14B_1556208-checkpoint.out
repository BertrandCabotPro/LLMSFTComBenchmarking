[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14d14a91f410>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14f305899fa0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x14d1739460c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14d14a91f410>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14d14ad81a30>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14d14a8540b0>, tokenizer='model')
functools.partial(<function finetune at 0x14f32e66a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14f305899fa0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14f305509c40>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14f305466630>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1469ee6cede0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1512d105fbc0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x146a175820c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1469ee6cede0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1469ee563f50>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1469ee46c410>, tokenizer='model')
functools.partial(<function finetune at 0x1512f9fba0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1512d105fbc0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1512d0dfbd70>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1512d0dfbb90>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14f5f2c2f8c0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x153009a7bbf0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x14f61bc1a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14f5f2c2f8c0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14f5f2aa77a0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14f5f2aa7560>, tokenizer='model')
functools.partial(<function finetune at 0x153032ab60c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x153009a7bbf0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1530098f7ce0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1530098f5e50>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1501e8bbef30>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14691a785580>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14dc777b7980>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14b2a848bad0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1484a0314950>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x149c0e2231d0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14c291f9af30>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x1501e84de0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1501e8bbef30>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x15055e2d4a10>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1501bf7036b0>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x1469435fa0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14691a785580>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14691a612e70>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14691a466e10>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x14dca07ba0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14dc777b7980>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14dc7762bb60>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14dc7762b920>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x146c76a4fe30>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1548c71d6b70>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x14b2d16e20c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14b2a848bad0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14b2a84ffe90>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14b2a8799340>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x1484a00d20c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1484a0314950>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x148476eefe00>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x148477165eb0>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x149c372360c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x149c0e2231d0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x149c0e09b740>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x149c0e09b500>, tokenizer='model')
functools.partial(<function finetune at 0x14c2bad420c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14c291f9af30>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14c291f71dc0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14c291ea8fe0>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1495f482b230>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x145e52debf80>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x146c9fb820c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x146c76a4fe30>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x146c76df0d70>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x146c769f3140>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14baa8cdfbc0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x1548effa20c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1548c71d6b70>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1548f01af500>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1548c71d49b0>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x1495f40ba0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1495f482b230>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1495caf10500>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1495cb2db470>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x145e7be2a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x145e52debf80>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x145e52ca4350>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x145e53082a20>, tokenizer='model')
functools.partial(<function finetune at 0x14baa855a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14baa8cdfbc0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14ba7f3367b0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14ba7f3df530>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14beb6eb3950>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14b2bfa53470>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1464bd17fe90>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x152ca7877440>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x14bee00960c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14beb6eb3950>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14beb6f27980>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14beb6f25460>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x153bd3060320>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x153105a16630>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14b5989bfdd0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x145b45393d70>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x14b2e8c2e0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14b2bfa53470>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14b2bfac3b60>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14b2bfa53170>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14ab2d0d7800>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14c0fff3b080>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x1464e61be0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1464bd17fe90>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1464e6afefc0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1464bd2e57f0>, tokenizer='model')
[NeMo I 2025-11-10 12:35:43 nemo_logging:393] Using passed HF dataset Dataset({
        features: ['id', 'messages', 'source'],
        num_rows: 939343
    })
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x149e76b846b0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x152cd044a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x152ca7877440>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x152ca747b200>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x152cd0d95340>, tokenizer='model')
functools.partial(<function finetune at 0x153bfbe0a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x153bd3060320>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x153bd2c6ffb0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x153bd2c6fd70>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1502c4a2bfe0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x151aaa394080>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x15312e8a20c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x153105a16630>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x153105b45eb0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x153105711970>, tokenizer='model')
functools.partial(<function finetune at 0x14b5c1baa0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14b5989bfdd0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14b598874140>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14b598a35af0>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1455d6227530>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1514e478faa0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x1457cf4e60c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x145b45393d70>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1457a6164080>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1457a6327e60>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x150750190fb0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x14ab55f720c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14ab2d0d7800>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14ab2cdc4b30>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14ab2cd94fe0>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x148a4a1f7800>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x14c1291120c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14c0fff3b080>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14c0fffab620>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14c0fffab470>, tokenizer='model')
functools.partial(<function finetune at 0x149e9fa122a0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x149e76b846b0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x149e76881100>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x149e76c829f0>, tokenizer='model')
[NeMo I 2025-11-10 12:35:43 nemo_logging:393] Experiments will be logged at ckpt_folder/Qwen/Qwen2.5-14B-Instruct/2025-11-10_12-35-43
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x1502edc060c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1502c4a2bfe0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1502c4e65bb0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1502eda227b0>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x151ad32c60c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x151aaa394080>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x151ad3c0b3b0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x151aaa1e0110>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x1455ff2620c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1455d6227530>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1455d6297590>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1455d62951c0>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14e4990831d0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14a5cdf2fef0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x15150d9c20c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1514e478faa0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1514e4803a70>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1514e4803830>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1468de934fb0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x15074fa1e0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x150750190fb0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1507266bc950>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x150726740200>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x148a7322e0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x148a4a1f7800>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x148a4a06fce0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x148a4a06c3b0>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14eb4200c7d0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x155050328620>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14e523e28200>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x14e4c207e0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14e4990831d0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14e4991dd400>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14e499082b70>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1549b372f500>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x14a5f6f2e0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14a5cdf2fef0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14a5cdda7da0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14a5cddac290>, tokenizer='model')
functools.partial(<function finetune at 0x1469077960c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1468de934fb0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1468de61ee70>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x146c7d6d2480>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1486a48a3dd0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x146bcb8fff20>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x14eb6b0de0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14eb4200c7d0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14eb420e31d0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14eb41ffbd70>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1537eeb68c20>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x1550501720c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x155050328620>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x155026fe3950>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x155026fe3740>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14776909c110>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x14e54cd6e0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14e523e28200>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14e523bc7b90>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14e523bc55b0>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14a9af3d31a0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14dab15b3b60>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14bb8a9a8a70>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x1549dc4e60c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1549b372f500>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1549b33812b0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1549b342eea0>, tokenizer='model')
functools.partial(<function finetune at 0x1486cdade0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1486a48a3dd0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1486a491cc50>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1486a47441a0>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x148f30db2570>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x146bf492e0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x146bcb8fff20>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x146bcb76f620>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x146bcb76f4a0>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x147007a88b00>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1473c0b2dc10>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x1537ee5de0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1537eeb68c20>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1537c55076b0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1537c540abd0>, tokenizer='model')
functools.partial(<function finetune at 0x147791e120c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14776909c110>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x147768c3ec90>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x147768c3ea80>, tokenizer='model')
functools.partial(<function finetune at 0x14a9d85f60c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14a9af3d31a0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14a9af422e10>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14a9af422c60>, tokenizer='model')
functools.partial(<function finetune at 0x14d73b64e0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14dab15b3b60>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14d7125ad370>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14d7124c3740>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14acff0a43e0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x14bbb377e0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14bb8a9a8a70>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14bbe110fa70>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14bb8a8543e0>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x146ef0af6840>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1538783c5df0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x15040bc13560>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x148bbaeee0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x148f30db2570>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x148b91d53410>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x148b91ee3140>, tokenizer='model')
functools.partial(<function finetune at 0x14700749a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x147007a88b00>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x146fde2dbef0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x146fde263590>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x152870249970>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x1473c0bfa0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1473c0b2dc10>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x147397a74620>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x147397bbb9b0>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1540bc8afdd0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x154ce7226f30>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x14acfe9a60c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14acff0a43e0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14acd5844290>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14acfe9ab500>, tokenizer='model')
functools.partial(<function finetune at 0x146f19c0a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x146ef0af6840>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x146f1a5410d0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x146ef0a07f50>, tokenizer='model')
functools.partial(<function finetune at 0x153877a7e0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1538783c5df0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1538957cce30>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1538783c7440>, tokenizer='model')
functools.partial(<function finetune at 0x150434c760c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x15040bc13560>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x15040bacc0b0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x15040bc8fe60>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1550cffbf200>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14b5b998cda0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x1528990060c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x152870249970>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x15286fef9490>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x15286fe44d70>, tokenizer='model')
functools.partial(<function finetune at 0x1540e5aba0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1540bc8afdd0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1540bc9605f0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1540bc921fa0>, tokenizer='model')
functools.partial(<function finetune at 0x154ce71360c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x154ce7226f30>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x154ce7a0dbb0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x154cbe25e030>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1498232ee930>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x1550f8ed60c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1550cffbf200>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1550cfd4f920>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1550cfed7590>, tokenizer='model')
functools.partial(<function finetune at 0x14b5e28260c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14b5b998cda0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14b5b967c1d0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14b5b9677fb0>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14e6cb6d16a0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x14984c4c20c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1498232ee930>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1498232ef500>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14982333eae0>, tokenizer='model')
<Partial[finetune(
  model=<Config[Qwen2Model(config=<Config[Qwen25Config14B()]>)]>,
  data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14cbbc1976e0>,
  trainer=<Config[Trainer(
    accelerator='gpu',
    strategy=<Config[MegatronStrategy(
      tensor_model_parallel_size=4,
      pipeline_model_parallel_size=4,
      virtual_pipeline_model_parallel_size=None,
      context_parallel_size=1,
      sequence_parallel=False,
      pipeline_dtype=torch.bfloat16,
      ckpt_load_strictness='log_all',
      gradient_as_bucket_view=True)]>,
    devices=4,
    num_nodes=16,
    callbacks=[],
    max_steps=100,
    limit_val_batches=None,
    limit_test_batches=None,
    val_check_interval=30,
    log_every_n_steps=10,
    accumulate_grad_batches=8,
    use_distributed_sampler=False,
    plugins=<Config[MegatronMixedPrecision(
      precision='bf16-mixed',
      params_dtype=torch.bfloat16,
      pipeline_dtype=torch.bfloat16,
      autocast_enabled=False,
      grad_reduce_in_fp32=True)]>)]>,
  log=<Config[NeMoLogger(
    name='Qwen/Qwen2.5-14B-Instruct',
    log_dir='ckpt_folder',
    ckpt=None,
    tensorboard=None,
    wandb=None)]>,
  resume=<Config[AutoResume(
    restore_config=<Config[RestoreConfig(path='nemo://Qwen/Qwen2.5-14B')]>)]>,
  optim=<Config[MegatronOptimizerModule(
    config=<Config[OptimizerConfig(
      optimizer='adam',
      lr=5e-06,
      weight_decay=0.1,
      fp16=False,
      bf16=True,
      adam_beta1=0.9,
      adam_beta2=0.98,
      adam_eps=1e-05,
      use_distributed_optimizer=True,
      clip_grad=1.0)]>,
    lr_scheduler=<Config[CosineAnnealingScheduler(warmup_steps=50, constant_steps=0, min_lr=0)]>)]>,
  tokenizer='model')]>
functools.partial(<function finetune at 0x14e6f47aa0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14e6cb6d16a0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14e6cb6243b0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14e6f4fd0ec0>, tokenizer='model')
functools.partial(<function finetune at 0x14cbe539e0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14cbbc1976e0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14cbbc20b9e0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14cbbc20b830>, tokenizer='model')
Mon Nov 10 12:35:45 CET 2025
