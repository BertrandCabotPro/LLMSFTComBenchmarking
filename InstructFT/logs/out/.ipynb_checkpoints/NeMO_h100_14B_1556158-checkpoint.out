[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x14bce3bf20c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14bcbaa13aa0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14bcbaa838f0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14bcbaa836e0>, tokenizer='model')
functools.partial(<function finetune at 0x150c2f7920c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x150c065b75f0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x150c06637500>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x150c064ec0e0>, tokenizer='model')
functools.partial(<function finetune at 0x14945420e0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14942b479280>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14942b1fbad0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14942b07f320>, tokenizer='model')
functools.partial(<function finetune at 0x14a89f6c60c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14a876663b60>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14a8764e3f80>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14a8764e1af0>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x14e4327760c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14e4095436e0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14e4095b4710>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14e409543830>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x1479482320c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14791f07f4a0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14791f0d2cc0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14791f0d2ab0>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x145e17d720c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x145deeb7be90>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x145deebf7b90>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x145deebf7920>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x154be0eb20c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x154be17c8c80>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x154bb7e73ce0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x154bb7cf3e30>, tokenizer='model')
functools.partial(<function finetune at 0x153a7cb9e0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x153a7d49a330>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x153a539ee030>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x153a53a00260>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x15158561a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x15155c51cda0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x15155c4bcb30>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x151585667fb0>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x14c810c5a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14c7e7b189e0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14c7e7ab02f0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14c8113c2ab0>, tokenizer='model')
functools.partial(<function finetune at 0x153d21ed60c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x153cf8eb7ef0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x153cf8d2fe60>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x153cf8d2fc20>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x148a0cd8a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1489e3e1bc80>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1489e3ba7ce0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1489e3ba7a70>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x14ab3fe020c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14ab16de3cb0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14ab16c5ff80>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14ab16c5fd70>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x14b33ee1a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14b315c3fce0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14b315caf8c0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14b315c3f650>, tokenizer='model')
functools.partial(<function finetune at 0x1537f966e0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1537d0903e90>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1537d04eb320>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1537d04e8920>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x15038cc820c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x150363a4f980>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x150363ac3d10>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x150363ac3ad0>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x1455855060c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1458fb487fb0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14555c1ae690>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14555c359ca0>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x15043443e0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x15040b2eef30>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x15040b2a7830>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x15040b2a75c0>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x14c23c7b60c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14c2139d5460>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14c2135e4920>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14c2139f80e0>, tokenizer='model')
functools.partial(<function finetune at 0x14efbaaaa0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14ef9186bdd0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14ef91924410>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14ef91cb78f0>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x149c97e4e0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x149c6ec4f9b0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x149c6f0a70b0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x149c6ec4e180>, tokenizer='model')
functools.partial(<function finetune at 0x14e6acaf20c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14e6ad40a210>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14e6838cb140>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14e68391e900>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x14fc8bf8a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14fc8c09f0e0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14fc62dae840>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14fc62e02d20>, tokenizer='model')
functools.partial(<function finetune at 0x1482e16620c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1482b8573770>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1482b84df260>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1482e18da0f0>, tokenizer='model')
functools.partial(<function finetune at 0x14548d0320c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x145463fefda0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x145463eac560>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1454640be000>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[NeMo I 2025-11-10 12:30:52 nemo_logging:393] Using passed HF dataset Dataset({
        features: ['id', 'messages', 'source'],
        num_rows: 939343
    })
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x151b6ef7a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x151b45f98fb0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x151b45dd0650>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x151b45d92240>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x145d6de020c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x145d44f50140>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x145d44c2e450>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x145d44f50500>, tokenizer='model')
[Megatron 3D] DP=4, TP=4, PP=4, CP=1 | world_size=64
GBS = 128 - DP=4, BS per Replica=32, PP_microBS=4, PP_nchunks=8
functools.partial(<function finetune at 0x14fbb498a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14fb8b86bc20>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14fb8b7dc710>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14fb8b6a8170>, tokenizer='model')
functools.partial(<function finetune at 0x15012a2ea0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1504a024fd10>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x15010112ee10>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x15010117ecf0>, tokenizer='model')
functools.partial(<function finetune at 0x150fa300e2a0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x150f7a00f8c0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x150f79e836b0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x150f79e834a0>, tokenizer='model')
[NeMo I 2025-11-10 12:30:52 nemo_logging:393] Experiments will be logged at ckpt_folder/Qwen/Qwen2.5-14B-Instruct/2025-11-10_12-30-52
functools.partial(<function finetune at 0x14fe8d22a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14fe63ff7680>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14fe8d7cd370>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14fe63f244a0>, tokenizer='model')
functools.partial(<function finetune at 0x14e23b6da0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14e2124fb140>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14e21256b860>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14e2125692b0>, tokenizer='model')
functools.partial(<function finetune at 0x14ceda7160c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14ceb14dbcb0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14ceb1557dd0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14ceb1557b90>, tokenizer='model')
functools.partial(<function finetune at 0x1480d7cca0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1480aecb3ef0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1480aeb2be00>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1480aecb36e0>, tokenizer='model')
functools.partial(<function finetune at 0x151fa99a20c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x151f809a33e0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x151f808139e0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x151f80813800>, tokenizer='model')
functools.partial(<function finetune at 0x14c3b97820c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14c390593f50>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14c390607f20>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14c390607cb0>, tokenizer='model')
functools.partial(<function finetune at 0x147fede1e0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x147fc504c2f0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x147fc4c6e660>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x147fc4c1f7d0>, tokenizer='model')
functools.partial(<function finetune at 0x151baca020c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x151bad1b5460>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x151b8385ac30>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x151b838ab140>, tokenizer='model')
functools.partial(<function finetune at 0x14b8b1dfe0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14b88913e780>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14b888c94470>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14b888d100e0>, tokenizer='model')
functools.partial(<function finetune at 0x14608ace20c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x146061be4410>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x146061b7ef30>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x146061b7ed80>, tokenizer='model')
functools.partial(<function finetune at 0x148f9b5a60c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x148f9bd338f0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x148f725c3590>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x148f725c33b0>, tokenizer='model')
functools.partial(<function finetune at 0x154db8ef20c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x154db9763920>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x154d8fd44a10>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x154d8fde8050>, tokenizer='model')
functools.partial(<function finetune at 0x14bf5871e0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14bf2f573140>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14bf5874e420>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14bf2f5c2d50>, tokenizer='model')
functools.partial(<function finetune at 0x1475ae80a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x147585c487a0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1475af138320>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1475856e01d0>, tokenizer='model')
functools.partial(<function finetune at 0x14e3103fa0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14e310d196d0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14e2e71bfdd0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14e3103260c0>, tokenizer='model')
functools.partial(<function finetune at 0x14d5c654a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14d59d983b90>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14d59d39bec0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14d59d39bce0>, tokenizer='model')
functools.partial(<function finetune at 0x1476339f20c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14760aa0eb40>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14760a862d20>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14760a862b10>, tokenizer='model')
functools.partial(<function finetune at 0x145a450ae0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x145a1c0bbce0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x145a1bf336b0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x145a1c127080>, tokenizer='model')
functools.partial(<function finetune at 0x14a4397be0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14a410783a40>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14a410638140>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14a4106c3770>, tokenizer='model')
functools.partial(<function finetune at 0x1458961ea0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14586d1afe00>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14586d02bfb0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14586d0286e0>, tokenizer='model')
functools.partial(<function finetune at 0x14adc18ca0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14ad9888fc50>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14ad9870bd70>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14ad9870bb00>, tokenizer='model')
functools.partial(<function finetune at 0x1521519420c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1521520a8da0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x15212878c260>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x15212878e1e0>, tokenizer='model')
functools.partial(<function finetune at 0x151c66eea0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x151c3dce7a10>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x151c3dd57bf0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x151c3dd579b0>, tokenizer='model')
functools.partial(<function finetune at 0x14769039a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1476675489b0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x147667232a80>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1476672328d0>, tokenizer='model')
functools.partial(<function finetune at 0x153fefcd60c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x153fc6af3a40>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x153fc6b73680>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x153fc6b73410>, tokenizer='model')
functools.partial(<function finetune at 0x153d3876a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x153d38c3f6b0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x153d0f5db650>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x153d0f5db470>, tokenizer='model')
functools.partial(<function finetune at 0x148e528de0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x148e298d6810>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x148e298d6cf0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x148e5323f6e0>, tokenizer='model')
functools.partial(<function finetune at 0x14605bb3a0c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14605c4763f0>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x146032d91f70>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14603297b590>, tokenizer='model')
functools.partial(<function finetune at 0x14d48a2520c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14d461570d10>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14d4610e3740>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14d4610e34d0>, tokenizer='model')
functools.partial(<function finetune at 0x152dae9460c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x152daf280e30>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x152d857a8ad0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x152d85a91fa0>, tokenizer='model')
functools.partial(<function finetune at 0x14a6c12d20c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x14a6980eb800>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14a69815fd10>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14a69815fad0>, tokenizer='model')
functools.partial(<function finetune at 0x14729da060c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1472748fac00>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x14729da2d2b0>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x14727489eff0>, tokenizer='model')
functools.partial(<function finetune at 0x1456d58420c0>, model=Qwen2Model(), data=<nemo.collections.llm.gpt.data.hf_dataset.HFDatasetDataModule object at 0x1456ac756150>, trainer=<nemo.lightning.pytorch.trainer.Trainer object at 0x1456ac755a30>, log=NeMoLogger(name='Qwen/Qwen2.5-14B-Instruct', log_dir='ckpt_folder', explicit_log_dir=None, version=None, use_datetime_version=True, log_local_rank_0_only=False, log_global_rank_0_only=False, files_to_copy=None, update_logger_directory=True, ckpt=None, tensorboard=None, wandb=None, extra_loggers=[]), resume=AutoResume(restore_config=RestoreConfig(path='nemo://Qwen/Qwen2.5-14B', load_model_state=True, load_optim_state=False, load_artifacts=True), resume_from_directory=None, resume_from_path=None, resume_if_exists=False, resume_past_end=False, resume_ignore_no_checkpoint=False), optim=<nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule object at 0x1456d5876420>, tokenizer='model')
Mon Nov 10 12:30:54 CET 2025
