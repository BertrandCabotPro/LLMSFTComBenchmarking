>>> Training on 64 processes
world size: 64, GBS: 128, BSperDev: 2, sequence length: 4096, selective AC ratio: 7/8, grad accumulation:1
number of parameters: 72706203648
Pre-loop Model MaxMemory for GPU:0 6.75679874420166 GBytes
global batch size: 128 - mini batch size: 2
DATALOADER 4 False True True 3 False 
Optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: True
    differentiable: False
    eps: 1e-05
    foreach: None
    fused: None
    lr: 1e-05
    maximize: False
    weight_decay: 0.1
)
Step 10 / 100 | Loss: 0.461 | Perplexity: inf | LR: 6.139e-08 | Wall: 102.2916808128357
Step 20 / 100 | Loss: 0.457 | Perplexity: inf | LR: 1.296e-07 | Wall: 108.31135654449463
Step 30 / 100 | Loss: 0.460 | Perplexity: inf | LR: 1.978e-07 | Wall: 108.64989042282104
Step 40 / 100 | Loss: 0.451 | Perplexity: inf | LR: 2.660e-07 | Wall: 108.65442323684692
Step 50 / 100 | Loss: 0.453 | Perplexity: inf | LR: 3.342e-07 | Wall: 108.30797123908997
Step 60 / 100 | Loss: 0.456 | Perplexity: inf | LR: 4.025e-07 | Wall: 108.97305178642273
Step 70 / 100 | Loss: 0.459 | Perplexity: inf | LR: 4.707e-07 | Wall: 108.14760422706604
Step 80 / 100 | Loss: 0.450 | Perplexity: inf | LR: 5.389e-07 | Wall: 109.88503909111023
Step 90 / 100 | Loss: 0.450 | Perplexity: inf | LR: 6.071e-07 | Wall: 109.57741808891296
Step 100 / 100 | Loss: 0.451 | Perplexity: inf | LR: 6.753e-07 | Wall: 109.28041553497314
>>> Training complete in: 0:18:12.222014
>>> Training performance time: min 10.016700029373169 avg 10.80462551598597 seconds (+/- 0.2967224231624312)
>>> Loading performance time: min 0.0001590251922607422 avg 0.007936569175334892 seconds (+/- 0.0027289052069709926)
>>> ########## BENCHMARKING #####################################
(Measured on 100 steps) Step time Avg : 10.80462551598597 s
Estimated Training Time (2 epochs bs 128) : 44.04685668683614 h
MaxMemory for GPU:0 60.8777232170105 GBytes
Sat Nov 15 12:11:52 CET 2025
