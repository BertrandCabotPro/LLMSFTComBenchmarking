Loading nemo/2.4.0
  Loading requirement: gcc/11.4.1 cuda/12.8.0 nccl/2.27.3-1-cuda
    cudnn/9.10.2.21-12-cuda openmpi/4.1.6-cuda intel-oneapi-mkl/2024.1
    magma/2.9.0-cuda sox/14.4.2 ffmpeg/6.1.1-cuda hdf5/1.12.0-mpi-cuda
    libjpeg-turbo/2.1.3
+ srun python -u nemo_TPFSDP2.py --model Qwen/Qwen2.5-14B-Instruct --use-te-optimizer --strategy fsdp2 --devices 4 --num-nodes 16 --tp-size 4 --dp-size 16 --batch-size 2
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[NeMo W 2025-11-09 13:45:45 nemo_logging:405] /lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lhotse/recipes/iwslt22_ta.py:323: SyntaxWarning: invalid escape sequence '\s'
      text = re.sub("\s+", " ", text)
    
[NeMo W 2025-11-09 13:45:45 nemo_logging:405] /lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lhotse/recipes/iwslt22_ta.py:324: SyntaxWarning: invalid escape sequence '\s'
      text = re.sub("\s+\.\s+", ".", text)
    
[NeMo W 2025-11-09 13:45:45 nemo_logging:405] 
    
    ************************************************************************************************************************
    ************************************************************************************************************************
    *****  Automodel on NVIDIA/NeMo is deprecated. Please, use https://github.com/NVIDIA-NeMo/Automodel repo instead.  *****
    ************************************************************************************************************************
    ************************************************************************************************************************
    
[NeMo W 2025-11-09 13:45:45 nemo_logging:405] Waiting for 2 seconds before this message disappears.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.1. Found flash-attn 2.8.3.
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.8.0)
    Python  3.12.10 (you have 3.12.10)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
[NeMo W 2025-11-09 13:46:08 nemo_logging:405] /lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/jieba/__init__.py:44: SyntaxWarning: invalid escape sequence '\.'
      re_han_default = re.compile("([\u4E00-\u9FD5a-zA-Z0-9+#&\._%\-]+)", re.U)
    
[NeMo W 2025-11-09 13:46:08 nemo_logging:405] /lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/jieba/__init__.py:46: SyntaxWarning: invalid escape sequence '\s'
      re_skip_default = re.compile("(\r\n|\s)", re.U)
    
[NeMo W 2025-11-09 13:46:08 nemo_logging:405] /lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/jieba/finalseg/__init__.py:78: SyntaxWarning: invalid escape sequence '\.'
      re_skip = re.compile("([a-zA-Z0-9]+(?:\.\d+)?%?)")
    
[NeMo W 2025-11-09 13:46:10 nemo_logging:405] /lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\('
      m = re.match('([su]([0-9]{1,2})p?) \(([0-9]{1,2}) bit\)$', token)
    
[NeMo W 2025-11-09 13:46:10 nemo_logging:405] /lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\('
      m2 = re.match('([su]([0-9]{1,2})p?)( \(default\))?$', token)
    
[NeMo W 2025-11-09 13:46:10 nemo_logging:405] /lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\('
      elif re.match('(flt)p?( \(default\))?$', token):
    
[NeMo W 2025-11-09 13:46:10 nemo_logging:405] /lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\('
      elif re.match('(dbl)p?( \(default\))?$', token):
    
[NeMo W 2025-11-09 13:46:13 nemo_logging:405] Dependency for linear CE loss is not available.                     Please refer to https://github.com/apple/ml-cross-entropy.
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[NeMo W 2025-11-09 13:46:13 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /tmp/tmpnzfxrt08
You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 64 processes
----------------------------------------------------------------------------------------------------

Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 137.01it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 158.64it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 170.21it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 159.65it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 164.09it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 151.47it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 146.39it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 154.06it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 157.03it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 150.76it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 152.17it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 150.39it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 153.55it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 150.22it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 147.14it/s]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 147.59it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 151.22it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 150.75it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 144.61it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 143.57it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 149.37it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 148.50it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 144.56it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 146.16it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 155.73it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 149.94it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 143.56it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 145.19it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 149.57it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 148.83it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 150.97it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 145.60it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 143.23it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 149.97it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 142.06it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 143.07it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 149.57it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 148.27it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 145.07it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 148.15it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 145.99it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 140.14it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 150.48it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 141.15it/s]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 145.11it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 141.53it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 138.72it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 145.86it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 145.27it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 143.67it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 138.86it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 138.03it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 139.31it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 142.70it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 142.94it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 144.93it/s]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 147.70it/s]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 146.11it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 139.84it/s]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 151.60it/s]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 153.73it/s]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 153.35it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 151.72it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 8/8 [00:00<00:00, 150.90it/s]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
[rank51]: Traceback (most recent call last):
[rank51]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank51]:     main()
[rank51]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank51]:     llm.api.finetune(
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank51]:     return train(
[rank51]:            ^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank51]:     trainer.fit(model, data)
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank51]:     call._call_and_handle_interrupt(
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank51]:     return trainer_fn(*args, **kwargs)
[rank51]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank51]:     self._run(model, ckpt_path=ckpt_path)
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank51]:     results = self._run_stage()
[rank51]:               ^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank51]:     self.fit_loop.run()
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank51]:     self.advance()
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank51]:     self.epoch_loop.run(self._data_fetcher)
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank51]:     self.advance(data_fetcher)
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank51]:     super().advance(data_fetcher)
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank51]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank51]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank51]:     self._optimizer_step(batch_idx, closure)
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank51]:     call._call_lightning_module_hook(
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank51]:     output = fn(*args, **kwargs)
[rank51]:              ^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank51]:     optimizer.step(closure=optimizer_closure)
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank51]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank51]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank51]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank51]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank51]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank51]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank51]:     return optimizer.step(closure=closure, **kwargs)
[rank51]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank51]:     out = func(*args, **kwargs)
[rank51]:           ^^^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank51]:     loss = closure()
[rank51]:            ^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank51]:     self._after_closure(model, optimizer)
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank51]:     self._clip_gradients(
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank51]:     call._call_lightning_module_hook(
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank51]:     output = fn(*args, **kwargs)
[rank51]:              ^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank51]:     self.clip_gradients(
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank51]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank51]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank51]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank51]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank51]:     return func(*args, **kwargs)
[rank51]:            ^^^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank51]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank51]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank51]:     return func(*args, **kwargs)
[rank51]:            ^^^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank51]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank51]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank51]:     return disable_fn(*args, **kwargs)
[rank51]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank51]:     return fn(*args, **kwargs)
[rank51]:            ^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank51]:     return DTensor._op_dispatcher.dispatch(
[rank51]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank51]:     self.sharding_propagator.propagate(op_info)
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank51]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank51]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank51]:     return self.cache(*args, **kwargs)
[rank51]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank51]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank51]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank51]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank51]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank51]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank51]:     raise ValueError(
[rank51]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63], mesh_dim_names=('data_parallel',)).
[rank46]: Traceback (most recent call last):
[rank46]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank46]:     main()
[rank46]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank46]:     llm.api.finetune(
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank46]:     return train(
[rank46]:            ^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank46]:     trainer.fit(model, data)
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank46]:     call._call_and_handle_interrupt(
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank46]:     return trainer_fn(*args, **kwargs)
[rank46]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank46]:     self._run(model, ckpt_path=ckpt_path)
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank46]:     results = self._run_stage()
[rank46]:               ^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank46]:     self.fit_loop.run()
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank46]:     self.advance()
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank46]:     self.epoch_loop.run(self._data_fetcher)
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank46]:     self.advance(data_fetcher)
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank46]:     super().advance(data_fetcher)
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank46]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank46]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank46]:     self._optimizer_step(batch_idx, closure)
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank46]:     call._call_lightning_module_hook(
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank46]:     output = fn(*args, **kwargs)
[rank46]:              ^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank46]:     optimizer.step(closure=optimizer_closure)
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank46]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank46]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank46]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank46]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank46]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank46]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank46]:     return optimizer.step(closure=closure, **kwargs)
[rank46]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank46]:     out = func(*args, **kwargs)
[rank46]:           ^^^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank46]:     loss = closure()
[rank46]:            ^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank46]:     self._after_closure(model, optimizer)
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank46]:     self._clip_gradients(
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank46]:     call._call_lightning_module_hook(
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank46]:     output = fn(*args, **kwargs)
[rank46]:              ^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank46]:     self.clip_gradients(
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank46]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank50]: Traceback (most recent call last):
[rank50]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank50]:     main()
[rank50]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank50]:     llm.api.finetune(
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank50]:     return train(
[rank50]:            ^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank50]:     trainer.fit(model, data)
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank50]:     call._call_and_handle_interrupt(
[rank46]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank50]:     return trainer_fn(*args, **kwargs)
[rank50]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank50]:     self._run(model, ckpt_path=ckpt_path)
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank50]:     results = self._run_stage()
[rank50]:               ^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank50]:     self.fit_loop.run()
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank46]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank46]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank46]:     return func(*args, **kwargs)
[rank46]:            ^^^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank46]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank46]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank46]:     return func(*args, **kwargs)
[rank46]:            ^^^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank46]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank46]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank46]:     return disable_fn(*args, **kwargs)
[rank46]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank46]:     return fn(*args, **kwargs)
[rank46]:            ^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank46]:     return DTensor._op_dispatcher.dispatch(
[rank46]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank46]:     self.sharding_propagator.propagate(op_info)
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank46]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank46]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank46]:     return self.cache(*args, **kwargs)
[rank46]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank46]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank46]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank50]:     self.advance()
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank50]:     self.epoch_loop.run(self._data_fetcher)
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank50]:     self.advance(data_fetcher)
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank50]:     super().advance(data_fetcher)
[rank46]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank46]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank50]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank50]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank50]:     self._optimizer_step(batch_idx, closure)
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank50]:     call._call_lightning_module_hook(
[rank46]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank46]:     raise ValueError(
[rank46]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62], mesh_dim_names=('data_parallel',)).
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank50]:     output = fn(*args, **kwargs)
[rank50]:              ^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank50]:     optimizer.step(closure=optimizer_closure)
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank50]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank50]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank50]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank50]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank50]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank50]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank50]:     return optimizer.step(closure=closure, **kwargs)
[rank50]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank50]:     out = func(*args, **kwargs)
[rank50]:           ^^^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank50]:     loss = closure()
[rank50]:            ^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank50]:     self._after_closure(model, optimizer)
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank50]:     self._clip_gradients(
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank50]:     call._call_lightning_module_hook(
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank50]:     output = fn(*args, **kwargs)
[rank50]:              ^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank50]:     self.clip_gradients(
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank50]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank50]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank47]: Traceback (most recent call last):
[rank47]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank47]:     main()
[rank47]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank47]:     llm.api.finetune(
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank47]:     return train(
[rank47]:            ^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank47]:     trainer.fit(model, data)
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank47]:     call._call_and_handle_interrupt(
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank50]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank50]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank50]:     return func(*args, **kwargs)
[rank50]:            ^^^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank47]:     return trainer_fn(*args, **kwargs)
[rank47]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank47]:     self._run(model, ckpt_path=ckpt_path)
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank47]:     results = self._run_stage()
[rank47]:               ^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank47]:     self.fit_loop.run()
[rank50]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank50]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank50]:     return func(*args, **kwargs)
[rank50]:            ^^^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank50]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank50]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank50]:     return disable_fn(*args, **kwargs)
[rank50]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank50]:     return fn(*args, **kwargs)
[rank50]:            ^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank50]:     return DTensor._op_dispatcher.dispatch(
[rank50]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank50]:     self.sharding_propagator.propagate(op_info)
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank50]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank50]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank50]:     return self.cache(*args, **kwargs)
[rank50]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank50]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank50]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank50]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank50]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank50]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank50]:     raise ValueError(
[rank50]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62], mesh_dim_names=('data_parallel',)).
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank47]:     self.advance()
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank47]:     self.epoch_loop.run(self._data_fetcher)
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank47]:     self.advance(data_fetcher)
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank47]:     super().advance(data_fetcher)
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank47]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank47]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank47]:     self._optimizer_step(batch_idx, closure)
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank47]:     call._call_lightning_module_hook(
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank47]:     output = fn(*args, **kwargs)
[rank47]:              ^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank47]:     optimizer.step(closure=optimizer_closure)
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank47]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank47]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank47]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank47]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank47]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank47]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank47]:     return optimizer.step(closure=closure, **kwargs)
[rank47]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank47]:     out = func(*args, **kwargs)
[rank47]:           ^^^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank47]:     loss = closure()
[rank47]:            ^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank47]:     self._after_closure(model, optimizer)
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank47]:     self._clip_gradients(
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank47]:     call._call_lightning_module_hook(
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank47]:     output = fn(*args, **kwargs)
[rank47]:              ^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank47]:     self.clip_gradients(
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank47]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank47]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank47]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank47]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank47]:     return func(*args, **kwargs)
[rank47]:            ^^^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank47]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank47]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank47]:     return func(*args, **kwargs)
[rank47]:            ^^^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank47]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank47]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank47]:     return disable_fn(*args, **kwargs)
[rank47]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank47]:     return fn(*args, **kwargs)
[rank47]:            ^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank47]:     return DTensor._op_dispatcher.dispatch(
[rank47]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank47]:     self.sharding_propagator.propagate(op_info)
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank47]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank47]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank47]:     return self.cache(*args, **kwargs)
[rank47]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank47]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank47]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank47]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank47]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank47]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank47]:     raise ValueError(
[rank47]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63], mesh_dim_names=('data_parallel',)).
[rank56]: Traceback (most recent call last):
[rank56]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank56]:     main()
[rank56]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank56]:     llm.api.finetune(
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank56]:     return train(
[rank56]:            ^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank56]:     trainer.fit(model, data)
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank56]:     call._call_and_handle_interrupt(
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank56]:     return trainer_fn(*args, **kwargs)
[rank56]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank56]:     self._run(model, ckpt_path=ckpt_path)
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank56]:     results = self._run_stage()
[rank56]:               ^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank56]:     self.fit_loop.run()
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank56]:     self.advance()
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank56]:     self.epoch_loop.run(self._data_fetcher)
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank56]:     self.advance(data_fetcher)
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank56]:     super().advance(data_fetcher)
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank56]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank56]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank56]:     self._optimizer_step(batch_idx, closure)
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank56]:     call._call_lightning_module_hook(
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank56]:     output = fn(*args, **kwargs)
[rank56]:              ^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank56]:     optimizer.step(closure=optimizer_closure)
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank56]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank56]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank56]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank56]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank56]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank56]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank56]:     return optimizer.step(closure=closure, **kwargs)
[rank56]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank56]:     out = func(*args, **kwargs)
[rank56]:           ^^^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank56]:     loss = closure()
[rank56]:            ^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank56]:     self._after_closure(model, optimizer)
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank56]:     self._clip_gradients(
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank56]:     call._call_lightning_module_hook(
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank56]:     output = fn(*args, **kwargs)
[rank56]:              ^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank56]:     self.clip_gradients(
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank56]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank56]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank56]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank56]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank56]:     return func(*args, **kwargs)
[rank56]:            ^^^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank56]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank56]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank56]:     return func(*args, **kwargs)
[rank56]:            ^^^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank56]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank56]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank56]:     return disable_fn(*args, **kwargs)
[rank56]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank56]:     return fn(*args, **kwargs)
[rank56]:            ^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank56]:     return DTensor._op_dispatcher.dispatch(
[rank56]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank56]:     self.sharding_propagator.propagate(op_info)
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank56]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank56]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank56]:     return self.cache(*args, **kwargs)
[rank56]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank56]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank56]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank56]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank56]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank56]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank56]:     raise ValueError(
[rank56]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60], mesh_dim_names=('data_parallel',)).
[rank57]: Traceback (most recent call last):
[rank57]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank57]:     main()
[rank57]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank57]:     llm.api.finetune(
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank57]:     return train(
[rank57]:            ^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank57]:     trainer.fit(model, data)
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank57]:     call._call_and_handle_interrupt(
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank57]:     return trainer_fn(*args, **kwargs)
[rank57]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank57]:     self._run(model, ckpt_path=ckpt_path)
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank57]:     results = self._run_stage()
[rank57]:               ^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank57]:     self.fit_loop.run()
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank57]:     self.advance()
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank57]:     self.epoch_loop.run(self._data_fetcher)
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank57]:     self.advance(data_fetcher)
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank57]:     super().advance(data_fetcher)
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank57]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank57]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank57]:     self._optimizer_step(batch_idx, closure)
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank57]:     call._call_lightning_module_hook(
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank57]:     output = fn(*args, **kwargs)
[rank57]:              ^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank57]:     optimizer.step(closure=optimizer_closure)
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank57]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank57]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank57]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank57]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank57]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank57]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank57]:     return optimizer.step(closure=closure, **kwargs)
[rank57]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank57]:     out = func(*args, **kwargs)
[rank57]:           ^^^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank57]:     loss = closure()
[rank57]:            ^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank57]:     self._after_closure(model, optimizer)
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank57]:     self._clip_gradients(
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank57]:     call._call_lightning_module_hook(
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank57]:     output = fn(*args, **kwargs)
[rank57]:              ^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank57]:     self.clip_gradients(
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank57]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank57]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank57]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank57]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank57]:     return func(*args, **kwargs)
[rank57]:            ^^^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank57]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank57]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank57]:     return func(*args, **kwargs)
[rank57]:            ^^^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank57]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank57]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank57]:     return disable_fn(*args, **kwargs)
[rank57]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank57]:     return fn(*args, **kwargs)
[rank57]:            ^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank57]:     return DTensor._op_dispatcher.dispatch(
[rank57]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank57]:     self.sharding_propagator.propagate(op_info)
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank57]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank57]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank57]:     return self.cache(*args, **kwargs)
[rank57]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank57]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank57]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank57]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank57]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank57]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank57]:     raise ValueError(
[rank57]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61], mesh_dim_names=('data_parallel',)).
[rank59]: Traceback (most recent call last):
[rank59]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank59]:     main()
[rank59]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank59]:     llm.api.finetune(
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank59]:     return train(
[rank59]:            ^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank59]:     trainer.fit(model, data)
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank59]:     call._call_and_handle_interrupt(
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank59]:     return trainer_fn(*args, **kwargs)
[rank59]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank59]:     self._run(model, ckpt_path=ckpt_path)
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank59]:     results = self._run_stage()
[rank59]:               ^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank59]:     self.fit_loop.run()
[rank44]: Traceback (most recent call last):
[rank44]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank44]:     main()
[rank44]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank44]:     llm.api.finetune(
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank44]:     return train(
[rank44]:            ^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank44]:     trainer.fit(model, data)
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank44]:     call._call_and_handle_interrupt(
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank59]:     self.advance()
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank59]:     self.epoch_loop.run(self._data_fetcher)
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank59]:     self.advance(data_fetcher)
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank59]:     super().advance(data_fetcher)
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank44]:     return trainer_fn(*args, **kwargs)
[rank44]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank44]:     self._run(model, ckpt_path=ckpt_path)
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank44]:     results = self._run_stage()
[rank44]:               ^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank44]:     self.fit_loop.run()
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank59]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank59]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank59]:     self._optimizer_step(batch_idx, closure)
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank59]:     call._call_lightning_module_hook(
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank44]:     self.advance()
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank44]:     self.epoch_loop.run(self._data_fetcher)
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank44]:     self.advance(data_fetcher)
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank44]:     super().advance(data_fetcher)
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank59]:     output = fn(*args, **kwargs)
[rank59]:              ^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank44]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank44]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank44]:     self._optimizer_step(batch_idx, closure)
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank44]:     call._call_lightning_module_hook(
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank59]:     optimizer.step(closure=optimizer_closure)
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank59]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank59]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank59]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank59]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank44]:     output = fn(*args, **kwargs)
[rank44]:              ^^^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank59]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank59]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank59]:     return optimizer.step(closure=closure, **kwargs)
[rank59]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank59]:     out = func(*args, **kwargs)
[rank59]:           ^^^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank44]:     optimizer.step(closure=optimizer_closure)
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank44]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank44]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank44]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank44]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank59]:     loss = closure()
[rank59]:            ^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank59]:     self._after_closure(model, optimizer)
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank59]:     self._clip_gradients(
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank59]:     call._call_lightning_module_hook(
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank44]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank44]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank44]:     return optimizer.step(closure=closure, **kwargs)
[rank44]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank44]:     out = func(*args, **kwargs)
[rank44]:           ^^^^^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank59]:     output = fn(*args, **kwargs)
[rank59]:              ^^^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank59]:     self.clip_gradients(
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank59]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank44]:     loss = closure()
[rank44]:            ^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank44]:     self._after_closure(model, optimizer)
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank44]:     self._clip_gradients(
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank44]:     call._call_lightning_module_hook(
[rank59]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank44]:     output = fn(*args, **kwargs)
[rank44]:              ^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank44]:     self.clip_gradients(
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank44]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank59]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank59]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank59]:     return func(*args, **kwargs)
[rank59]:            ^^^^^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank44]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank59]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank59]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank59]:     return func(*args, **kwargs)
[rank59]:            ^^^^^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank59]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank59]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank59]:     return disable_fn(*args, **kwargs)
[rank59]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank44]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank44]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank44]:     return func(*args, **kwargs)
[rank44]:            ^^^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank59]:     return fn(*args, **kwargs)
[rank59]:            ^^^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank59]:     return DTensor._op_dispatcher.dispatch(
[rank59]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank59]:     self.sharding_propagator.propagate(op_info)
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank44]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank44]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank44]:     return func(*args, **kwargs)
[rank44]:            ^^^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank44]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank44]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank44]:     return disable_fn(*args, **kwargs)
[rank44]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank59]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank59]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank59]:     return self.cache(*args, **kwargs)
[rank59]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank59]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank59]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank44]:     return fn(*args, **kwargs)
[rank44]:            ^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank44]:     return DTensor._op_dispatcher.dispatch(
[rank44]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank44]:     self.sharding_propagator.propagate(op_info)
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank59]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank59]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank44]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank44]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank44]:     return self.cache(*args, **kwargs)
[rank44]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank44]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank44]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank59]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank59]:     raise ValueError(
[rank59]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63], mesh_dim_names=('data_parallel',)).
[rank44]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank44]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank44]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank44]:     raise ValueError(
[rank44]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60], mesh_dim_names=('data_parallel',)).
[rank45]: Traceback (most recent call last):
[rank45]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank45]:     main()
[rank45]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank45]:     llm.api.finetune(
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank45]:     return train(
[rank45]:            ^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank45]:     trainer.fit(model, data)
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank45]:     call._call_and_handle_interrupt(
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank45]:     return trainer_fn(*args, **kwargs)
[rank45]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank45]:     self._run(model, ckpt_path=ckpt_path)
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank45]:     results = self._run_stage()
[rank45]:               ^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank45]:     self.fit_loop.run()
[rank58]: Traceback (most recent call last):
[rank58]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank58]:     main()
[rank58]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank58]:     llm.api.finetune(
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank58]:     return train(
[rank58]:            ^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank58]:     trainer.fit(model, data)
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank58]:     call._call_and_handle_interrupt(
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank45]:     self.advance()
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank45]:     self.epoch_loop.run(self._data_fetcher)
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank45]:     self.advance(data_fetcher)
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank45]:     super().advance(data_fetcher)
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank45]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank45]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank45]:     self._optimizer_step(batch_idx, closure)
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank45]:     call._call_lightning_module_hook(
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank58]:     return trainer_fn(*args, **kwargs)
[rank58]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank58]:     self._run(model, ckpt_path=ckpt_path)
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank58]:     results = self._run_stage()
[rank58]:               ^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank58]:     self.fit_loop.run()
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank45]:     output = fn(*args, **kwargs)
[rank45]:              ^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank58]:     self.advance()
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank58]:     self.epoch_loop.run(self._data_fetcher)
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank58]:     self.advance(data_fetcher)
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank58]:     super().advance(data_fetcher)
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank45]:     optimizer.step(closure=optimizer_closure)
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank45]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank45]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank45]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank45]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank58]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank58]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank58]:     self._optimizer_step(batch_idx, closure)
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank58]:     call._call_lightning_module_hook(
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank45]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank45]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank45]:     return optimizer.step(closure=closure, **kwargs)
[rank45]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank45]:     out = func(*args, **kwargs)
[rank45]:           ^^^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank58]:     output = fn(*args, **kwargs)
[rank58]:              ^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank45]:     loss = closure()
[rank45]:            ^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank45]:     self._after_closure(model, optimizer)
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank45]:     self._clip_gradients(
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank45]:     call._call_lightning_module_hook(
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank58]:     optimizer.step(closure=optimizer_closure)
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank58]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank58]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank58]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank58]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank45]:     output = fn(*args, **kwargs)
[rank45]:              ^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank45]:     self.clip_gradients(
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank45]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank58]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank58]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank58]:     return optimizer.step(closure=closure, **kwargs)
[rank58]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank58]:     out = func(*args, **kwargs)
[rank58]:           ^^^^^^^^^^^^^^^^^^^^^
[rank45]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank45]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank45]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank45]:     return func(*args, **kwargs)
[rank45]:            ^^^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank45]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank45]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank45]:     return func(*args, **kwargs)
[rank45]:            ^^^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank45]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank45]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank45]:     return disable_fn(*args, **kwargs)
[rank45]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank45]:     return fn(*args, **kwargs)
[rank45]:            ^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank45]:     return DTensor._op_dispatcher.dispatch(
[rank45]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank45]:     self.sharding_propagator.propagate(op_info)
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank45]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank45]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank45]:     return self.cache(*args, **kwargs)
[rank45]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank45]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank45]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank45]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank45]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank45]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank45]:     raise ValueError(
[rank45]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61], mesh_dim_names=('data_parallel',)).
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank58]:     loss = closure()
[rank58]:            ^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank58]:     self._after_closure(model, optimizer)
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank58]:     self._clip_gradients(
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank58]:     call._call_lightning_module_hook(
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank58]:     output = fn(*args, **kwargs)
[rank58]:              ^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank58]:     self.clip_gradients(
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank58]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank58]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank58]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank58]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank58]:     return func(*args, **kwargs)
[rank58]:            ^^^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank58]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank58]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank58]:     return func(*args, **kwargs)
[rank58]:            ^^^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank58]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank58]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank58]:     return disable_fn(*args, **kwargs)
[rank58]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank58]:     return fn(*args, **kwargs)
[rank58]:            ^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank58]:     return DTensor._op_dispatcher.dispatch(
[rank58]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank58]:     self.sharding_propagator.propagate(op_info)
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank58]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank58]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank58]:     return self.cache(*args, **kwargs)
[rank58]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank58]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank58]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank58]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank58]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank58]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank58]:     raise ValueError(
[rank58]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62], mesh_dim_names=('data_parallel',)).
[rank19]: Traceback (most recent call last):
[rank19]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank19]:     main()
[rank19]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank19]:     llm.api.finetune(
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank19]:     return train(
[rank19]:            ^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank19]:     trainer.fit(model, data)
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank19]:     call._call_and_handle_interrupt(
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank19]:     return trainer_fn(*args, **kwargs)
[rank19]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank19]:     self._run(model, ckpt_path=ckpt_path)
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank19]:     results = self._run_stage()
[rank19]:               ^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank19]:     self.fit_loop.run()
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank19]:     self.advance()
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank19]:     self.epoch_loop.run(self._data_fetcher)
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank19]:     self.advance(data_fetcher)
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank19]:     super().advance(data_fetcher)
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank19]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank19]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank19]:     self._optimizer_step(batch_idx, closure)
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank19]:     call._call_lightning_module_hook(
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank19]:     output = fn(*args, **kwargs)
[rank19]:              ^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank19]:     optimizer.step(closure=optimizer_closure)
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank19]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank19]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank19]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank19]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank19]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank19]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank19]:     return optimizer.step(closure=closure, **kwargs)
[rank19]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank19]:     out = func(*args, **kwargs)
[rank19]:           ^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank19]:     loss = closure()
[rank19]:            ^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank19]:     self._after_closure(model, optimizer)
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank19]:     self._clip_gradients(
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank19]:     call._call_lightning_module_hook(
[rank29]: Traceback (most recent call last):
[rank29]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank29]:     main()
[rank29]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank29]:     llm.api.finetune(
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank29]:     return train(
[rank29]:            ^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank29]:     trainer.fit(model, data)
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank29]:     call._call_and_handle_interrupt(
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank19]:     output = fn(*args, **kwargs)
[rank19]:              ^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank19]:     self.clip_gradients(
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank19]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank19]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank19]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank19]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank19]:     return func(*args, **kwargs)
[rank19]:            ^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank19]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank19]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank19]:     return func(*args, **kwargs)
[rank19]:            ^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank19]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank19]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank19]:     return disable_fn(*args, **kwargs)
[rank19]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank19]:     return fn(*args, **kwargs)
[rank19]:            ^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank19]:     return DTensor._op_dispatcher.dispatch(
[rank19]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank19]:     self.sharding_propagator.propagate(op_info)
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank19]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank19]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank19]:     return self.cache(*args, **kwargs)
[rank19]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank19]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank19]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank29]:     return trainer_fn(*args, **kwargs)
[rank29]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank29]:     self._run(model, ckpt_path=ckpt_path)
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank29]:     results = self._run_stage()
[rank29]:               ^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank29]:     self.fit_loop.run()
[rank19]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank19]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank29]:     self.advance()
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank29]:     self.epoch_loop.run(self._data_fetcher)
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank29]:     self.advance(data_fetcher)
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank29]:     super().advance(data_fetcher)
[rank19]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank19]:     raise ValueError(
[rank19]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63], mesh_dim_names=('data_parallel',)).
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank29]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank29]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank29]:     self._optimizer_step(batch_idx, closure)
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank29]:     call._call_lightning_module_hook(
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank29]:     output = fn(*args, **kwargs)
[rank29]:              ^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank29]:     optimizer.step(closure=optimizer_closure)
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank29]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank29]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank29]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank29]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank29]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank29]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank29]:     return optimizer.step(closure=closure, **kwargs)
[rank29]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank29]:     out = func(*args, **kwargs)
[rank29]:           ^^^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank29]:     loss = closure()
[rank29]:            ^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank29]:     self._after_closure(model, optimizer)
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank29]:     self._clip_gradients(
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank29]:     call._call_lightning_module_hook(
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank29]:     output = fn(*args, **kwargs)
[rank29]:              ^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank29]:     self.clip_gradients(
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank29]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank29]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank29]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank29]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank29]:     return func(*args, **kwargs)
[rank29]:            ^^^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank29]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank29]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank29]:     return func(*args, **kwargs)
[rank29]:            ^^^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank29]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank29]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank29]:     return disable_fn(*args, **kwargs)
[rank29]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank29]:     return fn(*args, **kwargs)
[rank29]:            ^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank29]:     return DTensor._op_dispatcher.dispatch(
[rank29]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank29]:     self.sharding_propagator.propagate(op_info)
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank29]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank29]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank29]:     return self.cache(*args, **kwargs)
[rank29]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank29]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank29]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank29]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank29]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank29]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank29]:     raise ValueError(
[rank29]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61], mesh_dim_names=('data_parallel',)).
[rank48]: Traceback (most recent call last):
[rank48]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank48]:     main()
[rank48]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank48]:     llm.api.finetune(
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank48]:     return train(
[rank48]:            ^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank48]:     trainer.fit(model, data)
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank48]:     call._call_and_handle_interrupt(
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank48]:     return trainer_fn(*args, **kwargs)
[rank48]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank48]:     self._run(model, ckpt_path=ckpt_path)
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank48]:     results = self._run_stage()
[rank48]:               ^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank48]:     self.fit_loop.run()
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank48]:     self.advance()
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank48]:     self.epoch_loop.run(self._data_fetcher)
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank48]:     self.advance(data_fetcher)
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank48]:     super().advance(data_fetcher)
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank48]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank48]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank48]:     self._optimizer_step(batch_idx, closure)
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank48]:     call._call_lightning_module_hook(
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank48]:     output = fn(*args, **kwargs)
[rank48]:              ^^^^^^^^^^^^^^^^^^^
[rank18]: Traceback (most recent call last):
[rank18]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank18]:     main()
[rank18]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank18]:     llm.api.finetune(
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank18]:     return train(
[rank18]:            ^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank18]:     trainer.fit(model, data)
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank18]:     call._call_and_handle_interrupt(
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank48]:     optimizer.step(closure=optimizer_closure)
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank48]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank48]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank48]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank48]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank18]:     return trainer_fn(*args, **kwargs)
[rank18]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank18]:     self._run(model, ckpt_path=ckpt_path)
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank18]:     results = self._run_stage()
[rank18]:               ^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank18]:     self.fit_loop.run()
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank48]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank48]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank48]:     return optimizer.step(closure=closure, **kwargs)
[rank48]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank48]:     out = func(*args, **kwargs)
[rank48]:           ^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank18]:     self.advance()
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank18]:     self.epoch_loop.run(self._data_fetcher)
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank18]:     self.advance(data_fetcher)
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank18]:     super().advance(data_fetcher)
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank18]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank18]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank18]:     self._optimizer_step(batch_idx, closure)
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank18]:     call._call_lightning_module_hook(
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank18]:     output = fn(*args, **kwargs)
[rank18]:              ^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank18]:     optimizer.step(closure=optimizer_closure)
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank18]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank18]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank18]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank18]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank18]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank18]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank18]:     return optimizer.step(closure=closure, **kwargs)
[rank18]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank18]:     out = func(*args, **kwargs)
[rank18]:           ^^^^^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank48]:     loss = closure()
[rank48]:            ^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank48]:     self._after_closure(model, optimizer)
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank48]:     self._clip_gradients(
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank48]:     call._call_lightning_module_hook(
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank18]:     loss = closure()
[rank18]:            ^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank18]:     self._after_closure(model, optimizer)
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank18]:     self._clip_gradients(
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank18]:     call._call_lightning_module_hook(
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank48]:     output = fn(*args, **kwargs)
[rank48]:              ^^^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank48]:     self.clip_gradients(
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank48]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank18]:     output = fn(*args, **kwargs)
[rank18]:              ^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank18]:     self.clip_gradients(
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank18]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank48]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank18]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank48]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank48]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank48]:     return func(*args, **kwargs)
[rank48]:            ^^^^^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank18]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank18]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank18]:     return func(*args, **kwargs)
[rank18]:            ^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank48]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank48]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank48]:     return func(*args, **kwargs)
[rank48]:            ^^^^^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank48]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank48]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank48]:     return disable_fn(*args, **kwargs)
[rank48]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank18]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank18]:     return func(*args, **kwargs)
[rank18]:            ^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank18]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank18]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank18]:     return disable_fn(*args, **kwargs)
[rank18]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank48]:     return fn(*args, **kwargs)
[rank48]:            ^^^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank48]:     return DTensor._op_dispatcher.dispatch(
[rank48]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank48]:     self.sharding_propagator.propagate(op_info)
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank18]:     return fn(*args, **kwargs)
[rank18]:            ^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank18]:     return DTensor._op_dispatcher.dispatch(
[rank18]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank18]:     self.sharding_propagator.propagate(op_info)
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank48]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank48]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank48]:     return self.cache(*args, **kwargs)
[rank48]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank48]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank48]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank18]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank18]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank18]:     return self.cache(*args, **kwargs)
[rank18]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank18]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank18]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank48]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank48]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank18]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank48]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank48]:     raise ValueError(
[rank48]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60], mesh_dim_names=('data_parallel',)).
[rank18]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank18]:     raise ValueError(
[rank18]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62], mesh_dim_names=('data_parallel',)).
[rank49]: Traceback (most recent call last):
[rank49]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank49]:     main()
[rank49]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank49]:     llm.api.finetune(
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank49]:     return train(
[rank49]:            ^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank49]:     trainer.fit(model, data)
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank49]:     call._call_and_handle_interrupt(
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank49]:     return trainer_fn(*args, **kwargs)
[rank49]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank49]:     self._run(model, ckpt_path=ckpt_path)
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank49]:     results = self._run_stage()
[rank49]:               ^^^^^^^^^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank49]:     self.fit_loop.run()
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank49]:     self.advance()
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank49]:     self.epoch_loop.run(self._data_fetcher)
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank49]:     self.advance(data_fetcher)
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank49]:     super().advance(data_fetcher)
[rank28]: Traceback (most recent call last):
[rank28]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank28]:     main()
[rank28]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank28]:     llm.api.finetune(
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank28]:     return train(
[rank28]:            ^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank28]:     trainer.fit(model, data)
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank28]:     call._call_and_handle_interrupt(
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank49]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank49]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank49]:     self._optimizer_step(batch_idx, closure)
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank49]:     call._call_lightning_module_hook(
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank28]:     return trainer_fn(*args, **kwargs)
[rank28]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank28]:     self._run(model, ckpt_path=ckpt_path)
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank28]:     results = self._run_stage()
[rank28]:               ^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank28]:     self.fit_loop.run()
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank49]:     output = fn(*args, **kwargs)
[rank49]:              ^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank28]:     self.advance()
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank28]:     self.epoch_loop.run(self._data_fetcher)
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank28]:     self.advance(data_fetcher)
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank28]:     super().advance(data_fetcher)
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank49]:     optimizer.step(closure=optimizer_closure)
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank49]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank49]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank49]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank49]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank28]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank28]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank28]:     self._optimizer_step(batch_idx, closure)
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank28]:     call._call_lightning_module_hook(
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank49]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank49]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank49]:     return optimizer.step(closure=closure, **kwargs)
[rank49]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank49]:     out = func(*args, **kwargs)
[rank49]:           ^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank28]:     output = fn(*args, **kwargs)
[rank28]:              ^^^^^^^^^^^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank49]:     loss = closure()
[rank49]:            ^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank49]:     self._after_closure(model, optimizer)
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank49]:     self._clip_gradients(
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank49]:     call._call_lightning_module_hook(
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank28]:     optimizer.step(closure=optimizer_closure)
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank28]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank28]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank28]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank28]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank49]:     output = fn(*args, **kwargs)
[rank49]:              ^^^^^^^^^^^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank49]:     self.clip_gradients(
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank49]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank28]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank28]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank28]:     return optimizer.step(closure=closure, **kwargs)
[rank28]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank28]:     out = func(*args, **kwargs)
[rank28]:           ^^^^^^^^^^^^^^^^^^^^^
[rank49]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank49]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank49]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank49]:     return func(*args, **kwargs)
[rank49]:            ^^^^^^^^^^^^^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank49]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank49]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank49]:     return func(*args, **kwargs)
[rank49]:            ^^^^^^^^^^^^^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank49]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank49]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank49]:     return disable_fn(*args, **kwargs)
[rank49]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank43]: Traceback (most recent call last):
[rank43]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank43]:     main()
[rank43]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank43]:     llm.api.finetune(
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank43]:     return train(
[rank43]:            ^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank43]:     trainer.fit(model, data)
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank43]:     call._call_and_handle_interrupt(
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank28]:     loss = closure()
[rank28]:            ^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank28]:     self._after_closure(model, optimizer)
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank28]:     self._clip_gradients(
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank28]:     call._call_lightning_module_hook(
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank49]:     return fn(*args, **kwargs)
[rank49]:            ^^^^^^^^^^^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank49]:     return DTensor._op_dispatcher.dispatch(
[rank49]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank49]:     self.sharding_propagator.propagate(op_info)
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank43]:     return trainer_fn(*args, **kwargs)
[rank43]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank43]:     self._run(model, ckpt_path=ckpt_path)
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank43]:     results = self._run_stage()
[rank43]:               ^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank43]:     self.fit_loop.run()
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank28]:     output = fn(*args, **kwargs)
[rank28]:              ^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank28]:     self.clip_gradients(
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank28]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank49]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank49]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank49]:     return self.cache(*args, **kwargs)
[rank49]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank49]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank49]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank43]:     self.advance()
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank43]:     self.epoch_loop.run(self._data_fetcher)
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank43]:     self.advance(data_fetcher)
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank43]:     super().advance(data_fetcher)
[rank28]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank49]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank49]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank43]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank43]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank43]:     self._optimizer_step(batch_idx, closure)
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank43]:     call._call_lightning_module_hook(
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank28]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank28]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank28]:     return func(*args, **kwargs)
[rank28]:            ^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank49]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank49]:     raise ValueError(
[rank49]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61], mesh_dim_names=('data_parallel',)).
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank43]:     output = fn(*args, **kwargs)
[rank43]:              ^^^^^^^^^^^^^^^^^^^
[rank28]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank28]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank28]:     return func(*args, **kwargs)
[rank28]:            ^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank28]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank28]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank28]:     return disable_fn(*args, **kwargs)
[rank28]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank43]:     optimizer.step(closure=optimizer_closure)
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank43]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank43]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank43]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank43]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank28]:     return fn(*args, **kwargs)
[rank28]:            ^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank28]:     return DTensor._op_dispatcher.dispatch(
[rank28]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank28]:     self.sharding_propagator.propagate(op_info)
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank43]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank43]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank43]:     return optimizer.step(closure=closure, **kwargs)
[rank43]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank43]:     out = func(*args, **kwargs)
[rank43]:           ^^^^^^^^^^^^^^^^^^^^^
[rank28]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank28]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank28]:     return self.cache(*args, **kwargs)
[rank28]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank28]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank28]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank43]:     loss = closure()
[rank43]:            ^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank43]:     self._after_closure(model, optimizer)
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank43]:     self._clip_gradients(
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank43]:     call._call_lightning_module_hook(
[rank28]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank28]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank43]:     output = fn(*args, **kwargs)
[rank43]:              ^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank43]:     self.clip_gradients(
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank43]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank28]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank28]:     raise ValueError(
[rank28]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60], mesh_dim_names=('data_parallel',)).
[rank43]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank43]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank43]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank43]:     return func(*args, **kwargs)
[rank43]:            ^^^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank43]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank43]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank43]:     return func(*args, **kwargs)
[rank43]:            ^^^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank43]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank43]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank43]:     return disable_fn(*args, **kwargs)
[rank43]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank43]:     return fn(*args, **kwargs)
[rank43]:            ^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank43]:     return DTensor._op_dispatcher.dispatch(
[rank43]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank43]:     self.sharding_propagator.propagate(op_info)
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank43]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank43]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank43]:     return self.cache(*args, **kwargs)
[rank43]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank43]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank43]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank43]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank43]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank43]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank43]:     raise ValueError(
[rank43]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63], mesh_dim_names=('data_parallel',)).
[rank54]: Traceback (most recent call last):
[rank54]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank54]:     main()
[rank54]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank54]:     llm.api.finetune(
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank54]:     return train(
[rank54]:            ^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank54]:     trainer.fit(model, data)
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank54]:     call._call_and_handle_interrupt(
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank54]:     return trainer_fn(*args, **kwargs)
[rank54]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank54]:     self._run(model, ckpt_path=ckpt_path)
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank54]:     results = self._run_stage()
[rank54]:               ^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank54]:     self.fit_loop.run()
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank54]:     self.advance()
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank54]:     self.epoch_loop.run(self._data_fetcher)
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank54]:     self.advance(data_fetcher)
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank54]:     super().advance(data_fetcher)
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank54]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank54]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank54]:     self._optimizer_step(batch_idx, closure)
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank54]:     call._call_lightning_module_hook(
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank54]:     output = fn(*args, **kwargs)
[rank54]:              ^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank54]:     optimizer.step(closure=optimizer_closure)
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank54]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank54]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank54]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank54]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank54]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank54]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank54]:     return optimizer.step(closure=closure, **kwargs)
[rank54]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank54]:     out = func(*args, **kwargs)
[rank54]:           ^^^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank54]:     loss = closure()
[rank54]:            ^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank54]:     self._after_closure(model, optimizer)
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank54]:     self._clip_gradients(
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank54]:     call._call_lightning_module_hook(
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank54]:     output = fn(*args, **kwargs)
[rank54]:              ^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank54]:     self.clip_gradients(
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank54]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank54]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank54]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank54]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank54]:     return func(*args, **kwargs)
[rank54]:            ^^^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank54]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank54]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank54]:     return func(*args, **kwargs)
[rank54]:            ^^^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank54]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank54]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank54]:     return disable_fn(*args, **kwargs)
[rank54]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank54]:     return fn(*args, **kwargs)
[rank54]:            ^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank54]:     return DTensor._op_dispatcher.dispatch(
[rank54]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank54]:     self.sharding_propagator.propagate(op_info)
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank54]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank54]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank54]:     return self.cache(*args, **kwargs)
[rank54]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank54]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank54]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank54]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank54]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank54]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank54]:     raise ValueError(
[rank54]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62], mesh_dim_names=('data_parallel',)).
[rank53]: Traceback (most recent call last):
[rank53]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank53]:     main()
[rank53]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank53]:     llm.api.finetune(
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank53]:     return train(
[rank53]:            ^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank53]:     trainer.fit(model, data)
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank53]:     call._call_and_handle_interrupt(
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank53]:     return trainer_fn(*args, **kwargs)
[rank53]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank53]:     self._run(model, ckpt_path=ckpt_path)
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank53]:     results = self._run_stage()
[rank53]:               ^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank53]:     self.fit_loop.run()
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank53]:     self.advance()
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank53]:     self.epoch_loop.run(self._data_fetcher)
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank53]:     self.advance(data_fetcher)
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank53]:     super().advance(data_fetcher)
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank53]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank53]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank53]:     self._optimizer_step(batch_idx, closure)
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank53]:     call._call_lightning_module_hook(
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank53]:     output = fn(*args, **kwargs)
[rank53]:              ^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank53]:     optimizer.step(closure=optimizer_closure)
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank53]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank53]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank53]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank53]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank53]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank53]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank53]:     return optimizer.step(closure=closure, **kwargs)
[rank53]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank53]:     out = func(*args, **kwargs)
[rank53]:           ^^^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank53]:     loss = closure()
[rank53]:            ^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank53]:     self._after_closure(model, optimizer)
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank53]:     self._clip_gradients(
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank53]:     call._call_lightning_module_hook(
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank53]:     output = fn(*args, **kwargs)
[rank53]:              ^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank53]:     self.clip_gradients(
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank53]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank53]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank53]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank53]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank53]:     return func(*args, **kwargs)
[rank53]:            ^^^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank53]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank53]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank53]:     return func(*args, **kwargs)
[rank53]:            ^^^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank53]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank53]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank53]:     return disable_fn(*args, **kwargs)
[rank53]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank53]:     return fn(*args, **kwargs)
[rank53]:            ^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank53]:     return DTensor._op_dispatcher.dispatch(
[rank53]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank53]:     self.sharding_propagator.propagate(op_info)
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank53]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank53]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank53]:     return self.cache(*args, **kwargs)
[rank53]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank53]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank53]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank53]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank53]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank53]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank53]:     raise ValueError(
[rank53]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61], mesh_dim_names=('data_parallel',)).
[rank6]: Traceback (most recent call last):
[rank6]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank6]:     main()
[rank6]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank6]:     llm.api.finetune(
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank6]:     return train(
[rank6]:            ^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank6]:     trainer.fit(model, data)
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank6]:     call._call_and_handle_interrupt(
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank6]:     return trainer_fn(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank6]:     self._run(model, ckpt_path=ckpt_path)
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank6]:     results = self._run_stage()
[rank6]:               ^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank6]:     self.fit_loop.run()
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank6]:     self.advance()
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank6]:     self.epoch_loop.run(self._data_fetcher)
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank6]:     self.advance(data_fetcher)
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank6]:     super().advance(data_fetcher)
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank6]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank6]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank6]:     self._optimizer_step(batch_idx, closure)
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank6]:     call._call_lightning_module_hook(
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank6]:     output = fn(*args, **kwargs)
[rank6]:              ^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank6]:     optimizer.step(closure=optimizer_closure)
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank6]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank6]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank6]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank6]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank6]:     return optimizer.step(closure=closure, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank6]:     out = func(*args, **kwargs)
[rank6]:           ^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank6]:     loss = closure()
[rank6]:            ^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank6]:     self._after_closure(model, optimizer)
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank6]:     self._clip_gradients(
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank6]:     call._call_lightning_module_hook(
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank6]:     output = fn(*args, **kwargs)
[rank6]:              ^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank6]:     self.clip_gradients(
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank6]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank6]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank6]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank6]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank6]:     return func(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank6]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank6]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank6]:     return func(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank6]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank6]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank6]:     return disable_fn(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank6]:     return fn(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank6]:     return DTensor._op_dispatcher.dispatch(
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank6]:     self.sharding_propagator.propagate(op_info)
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank6]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank6]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank6]:     return self.cache(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank6]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank6]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank6]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank6]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank6]:     raise ValueError(
[rank6]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62], mesh_dim_names=('data_parallel',)).
[rank32]: Traceback (most recent call last):
[rank32]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank32]:     main()
[rank32]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank32]:     llm.api.finetune(
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank32]:     return train(
[rank32]:            ^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank32]:     trainer.fit(model, data)
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank32]:     call._call_and_handle_interrupt(
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank32]:     return trainer_fn(*args, **kwargs)
[rank32]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank32]:     self._run(model, ckpt_path=ckpt_path)
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank32]:     results = self._run_stage()
[rank32]:               ^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank32]:     self.fit_loop.run()
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank32]:     self.advance()
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank32]:     self.epoch_loop.run(self._data_fetcher)
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank32]:     self.advance(data_fetcher)
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank32]:     super().advance(data_fetcher)
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank32]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank32]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank32]:     self._optimizer_step(batch_idx, closure)
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank32]:     call._call_lightning_module_hook(
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank32]:     output = fn(*args, **kwargs)
[rank32]:              ^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank32]:     optimizer.step(closure=optimizer_closure)
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank32]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank32]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank32]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank32]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank32]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank32]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank32]:     return optimizer.step(closure=closure, **kwargs)
[rank32]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank32]:     out = func(*args, **kwargs)
[rank32]:           ^^^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank32]:     loss = closure()
[rank32]:            ^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank32]:     self._after_closure(model, optimizer)
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank32]:     self._clip_gradients(
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank32]:     call._call_lightning_module_hook(
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank32]:     output = fn(*args, **kwargs)
[rank32]:              ^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank32]:     self.clip_gradients(
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank32]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank32]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank55]: Traceback (most recent call last):
[rank55]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank55]:     main()
[rank55]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank55]:     llm.api.finetune(
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank55]:     return train(
[rank55]:            ^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank55]:     trainer.fit(model, data)
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank55]:     call._call_and_handle_interrupt(
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank32]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank32]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank32]:     return func(*args, **kwargs)
[rank32]:            ^^^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank32]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank32]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank32]:     return func(*args, **kwargs)
[rank32]:            ^^^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank32]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank32]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank32]:     return disable_fn(*args, **kwargs)
[rank32]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank55]:     return trainer_fn(*args, **kwargs)
[rank55]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank55]:     self._run(model, ckpt_path=ckpt_path)
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank55]:     results = self._run_stage()
[rank55]:               ^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank55]:     self.fit_loop.run()
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank32]:     return fn(*args, **kwargs)
[rank32]:            ^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank32]:     return DTensor._op_dispatcher.dispatch(
[rank32]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank32]:     self.sharding_propagator.propagate(op_info)
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank55]:     self.advance()
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank55]:     self.epoch_loop.run(self._data_fetcher)
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank55]:     self.advance(data_fetcher)
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank55]:     super().advance(data_fetcher)
[rank32]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank32]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank32]:     return self.cache(*args, **kwargs)
[rank32]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank32]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank32]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank55]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank55]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank55]:     self._optimizer_step(batch_idx, closure)
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank55]:     call._call_lightning_module_hook(
[rank32]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank32]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank55]:     output = fn(*args, **kwargs)
[rank55]:              ^^^^^^^^^^^^^^^^^^^
[rank32]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank32]:     raise ValueError(
[rank32]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60], mesh_dim_names=('data_parallel',)).
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank55]:     optimizer.step(closure=optimizer_closure)
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank55]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank55]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank55]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank55]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank55]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank55]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank55]:     return optimizer.step(closure=closure, **kwargs)
[rank55]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank55]:     out = func(*args, **kwargs)
[rank55]:           ^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank55]:     loss = closure()
[rank55]:            ^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank55]:     self._after_closure(model, optimizer)
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank55]:     self._clip_gradients(
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank55]:     call._call_lightning_module_hook(
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank55]:     output = fn(*args, **kwargs)
[rank55]:              ^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank55]:     self.clip_gradients(
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank55]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank55]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank55]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank55]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank55]:     return func(*args, **kwargs)
[rank55]:            ^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank55]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank55]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank55]:     return func(*args, **kwargs)
[rank55]:            ^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank55]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank55]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank55]:     return disable_fn(*args, **kwargs)
[rank55]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank55]:     return fn(*args, **kwargs)
[rank55]:            ^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank55]:     return DTensor._op_dispatcher.dispatch(
[rank55]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank55]:     self.sharding_propagator.propagate(op_info)
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank55]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank55]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank55]:     return self.cache(*args, **kwargs)
[rank55]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank55]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank55]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank55]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank55]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank55]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank55]:     raise ValueError(
[rank55]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63], mesh_dim_names=('data_parallel',)).
[rank52]: Traceback (most recent call last):
[rank52]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank52]:     main()
[rank52]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank52]:     llm.api.finetune(
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank52]:     return train(
[rank52]:            ^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank52]:     trainer.fit(model, data)
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank52]:     call._call_and_handle_interrupt(
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank52]:     return trainer_fn(*args, **kwargs)
[rank52]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank52]:     self._run(model, ckpt_path=ckpt_path)
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank52]:     results = self._run_stage()
[rank52]:               ^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank52]:     self.fit_loop.run()
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank52]:     self.advance()
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank52]:     self.epoch_loop.run(self._data_fetcher)
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank52]:     self.advance(data_fetcher)
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank52]:     super().advance(data_fetcher)
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank52]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank52]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank52]:     self._optimizer_step(batch_idx, closure)
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank52]:     call._call_lightning_module_hook(
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank52]:     output = fn(*args, **kwargs)
[rank52]:              ^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank52]:     optimizer.step(closure=optimizer_closure)
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank52]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank52]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank52]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank52]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank52]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank52]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank52]:     return optimizer.step(closure=closure, **kwargs)
[rank52]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank52]:     out = func(*args, **kwargs)
[rank52]:           ^^^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank52]:     loss = closure()
[rank52]:            ^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank52]:     self._after_closure(model, optimizer)
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank52]:     self._clip_gradients(
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank52]:     call._call_lightning_module_hook(
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank52]:     output = fn(*args, **kwargs)
[rank52]:              ^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank52]:     self.clip_gradients(
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank52]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank52]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank52]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank52]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank52]:     return func(*args, **kwargs)
[rank52]:            ^^^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank52]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank52]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank52]:     return func(*args, **kwargs)
[rank52]:            ^^^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank52]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank52]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank52]:     return disable_fn(*args, **kwargs)
[rank52]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank52]:     return fn(*args, **kwargs)
[rank52]:            ^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank52]:     return DTensor._op_dispatcher.dispatch(
[rank52]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank52]:     self.sharding_propagator.propagate(op_info)
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank52]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank52]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank52]:     return self.cache(*args, **kwargs)
[rank52]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank52]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank52]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank52]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank52]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank52]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank52]:     raise ValueError(
[rank52]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60], mesh_dim_names=('data_parallel',)).
[rank33]: Traceback (most recent call last):
[rank33]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank33]:     main()
[rank33]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank33]:     llm.api.finetune(
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank33]:     return train(
[rank33]:            ^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank33]:     trainer.fit(model, data)
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank33]:     call._call_and_handle_interrupt(
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank33]:     return trainer_fn(*args, **kwargs)
[rank33]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank33]:     self._run(model, ckpt_path=ckpt_path)
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank33]:     results = self._run_stage()
[rank33]:               ^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank33]:     self.fit_loop.run()
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank33]:     self.advance()
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank33]:     self.epoch_loop.run(self._data_fetcher)
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank33]:     self.advance(data_fetcher)
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank33]:     super().advance(data_fetcher)
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank33]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank33]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank33]:     self._optimizer_step(batch_idx, closure)
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank33]:     call._call_lightning_module_hook(
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank33]:     output = fn(*args, **kwargs)
[rank33]:              ^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank33]:     optimizer.step(closure=optimizer_closure)
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank33]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank33]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank33]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank33]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank33]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank33]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank33]:     return optimizer.step(closure=closure, **kwargs)
[rank33]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank33]:     out = func(*args, **kwargs)
[rank33]:           ^^^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank33]:     loss = closure()
[rank33]:            ^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank33]:     self._after_closure(model, optimizer)
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank33]:     self._clip_gradients(
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank33]:     call._call_lightning_module_hook(
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank33]:     output = fn(*args, **kwargs)
[rank33]:              ^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank33]:     self.clip_gradients(
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank33]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank33]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank33]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank33]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank33]:     return func(*args, **kwargs)
[rank33]:            ^^^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank33]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank33]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank33]:     return func(*args, **kwargs)
[rank33]:            ^^^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank33]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank33]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank33]:     return disable_fn(*args, **kwargs)
[rank33]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank33]:     return fn(*args, **kwargs)
[rank33]:            ^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank33]:     return DTensor._op_dispatcher.dispatch(
[rank33]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank33]:     self.sharding_propagator.propagate(op_info)
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank33]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank33]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank33]:     return self.cache(*args, **kwargs)
[rank33]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank33]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank33]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank33]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank33]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank33]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank33]:     raise ValueError(
[rank33]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61], mesh_dim_names=('data_parallel',)).
[rank42]: Traceback (most recent call last):
[rank42]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank42]:     main()
[rank42]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank42]:     llm.api.finetune(
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank42]:     return train(
[rank42]:            ^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank42]:     trainer.fit(model, data)
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank42]:     call._call_and_handle_interrupt(
[rank34]: Traceback (most recent call last):
[rank34]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank34]:     main()
[rank34]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank34]:     llm.api.finetune(
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank34]:     return train(
[rank34]:            ^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank34]:     trainer.fit(model, data)
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank34]:     call._call_and_handle_interrupt(
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank34]:     return trainer_fn(*args, **kwargs)
[rank34]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank34]:     self._run(model, ckpt_path=ckpt_path)
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank34]:     results = self._run_stage()
[rank34]:               ^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank34]:     self.fit_loop.run()
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank42]:     return trainer_fn(*args, **kwargs)
[rank42]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank42]:     self._run(model, ckpt_path=ckpt_path)
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank42]:     results = self._run_stage()
[rank42]:               ^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank42]:     self.fit_loop.run()
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank42]:     self.advance()
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank42]:     self.epoch_loop.run(self._data_fetcher)
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank42]:     self.advance(data_fetcher)
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank42]:     super().advance(data_fetcher)
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank42]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank42]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank42]:     self._optimizer_step(batch_idx, closure)
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank42]:     call._call_lightning_module_hook(
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank34]:     self.advance()
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank34]:     self.epoch_loop.run(self._data_fetcher)
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank34]:     self.advance(data_fetcher)
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank34]:     super().advance(data_fetcher)
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank42]:     output = fn(*args, **kwargs)
[rank42]:              ^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank34]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank34]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank34]:     self._optimizer_step(batch_idx, closure)
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank34]:     call._call_lightning_module_hook(
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank42]:     optimizer.step(closure=optimizer_closure)
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank42]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank42]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank42]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank42]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank34]:     output = fn(*args, **kwargs)
[rank34]:              ^^^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank42]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank42]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank42]:     return optimizer.step(closure=closure, **kwargs)
[rank42]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank42]:     out = func(*args, **kwargs)
[rank42]:           ^^^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank34]:     optimizer.step(closure=optimizer_closure)
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank34]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank34]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank34]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank34]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank42]:     loss = closure()
[rank42]:            ^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank42]:     self._after_closure(model, optimizer)
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank42]:     self._clip_gradients(
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank42]:     call._call_lightning_module_hook(
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank34]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank34]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank34]:     return optimizer.step(closure=closure, **kwargs)
[rank34]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank34]:     out = func(*args, **kwargs)
[rank34]:           ^^^^^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank42]:     output = fn(*args, **kwargs)
[rank42]:              ^^^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank42]:     self.clip_gradients(
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank42]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank34]:     loss = closure()
[rank34]:            ^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank34]:     self._after_closure(model, optimizer)
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank34]:     self._clip_gradients(
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank34]:     call._call_lightning_module_hook(
[rank42]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank34]:     output = fn(*args, **kwargs)
[rank34]:              ^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank34]:     self.clip_gradients(
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank34]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank42]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank42]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank42]:     return func(*args, **kwargs)
[rank42]:            ^^^^^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank34]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank42]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank42]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank42]:     return func(*args, **kwargs)
[rank42]:            ^^^^^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank42]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank42]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank42]:     return disable_fn(*args, **kwargs)
[rank42]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank34]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank34]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank34]:     return func(*args, **kwargs)
[rank34]:            ^^^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank42]:     return fn(*args, **kwargs)
[rank42]:            ^^^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank42]:     return DTensor._op_dispatcher.dispatch(
[rank42]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank42]:     self.sharding_propagator.propagate(op_info)
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank34]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank34]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank34]:     return func(*args, **kwargs)
[rank34]:            ^^^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank34]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank34]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank34]:     return disable_fn(*args, **kwargs)
[rank34]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank42]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank42]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank42]:     return self.cache(*args, **kwargs)
[rank42]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank42]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank42]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank34]:     return fn(*args, **kwargs)
[rank34]:            ^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank34]:     return DTensor._op_dispatcher.dispatch(
[rank34]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank34]:     self.sharding_propagator.propagate(op_info)
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank42]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank42]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank34]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank34]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank34]:     return self.cache(*args, **kwargs)
[rank34]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank34]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank34]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank42]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank42]:     raise ValueError(
[rank42]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62], mesh_dim_names=('data_parallel',)).
[rank34]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank34]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank34]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank34]:     raise ValueError(
[rank34]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62], mesh_dim_names=('data_parallel',)).
[rank35]: Traceback (most recent call last):
[rank35]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank35]:     main()
[rank35]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank35]:     llm.api.finetune(
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank35]:     return train(
[rank35]:            ^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank35]:     trainer.fit(model, data)
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank35]:     call._call_and_handle_interrupt(
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank35]:     return trainer_fn(*args, **kwargs)
[rank35]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank35]:     self._run(model, ckpt_path=ckpt_path)
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank35]:     results = self._run_stage()
[rank35]:               ^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank35]:     self.fit_loop.run()
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank35]:     self.advance()
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank35]:     self.epoch_loop.run(self._data_fetcher)
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank35]:     self.advance(data_fetcher)
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank35]:     super().advance(data_fetcher)
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank35]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank35]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank35]:     self._optimizer_step(batch_idx, closure)
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank35]:     call._call_lightning_module_hook(
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank35]:     output = fn(*args, **kwargs)
[rank35]:              ^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank35]:     optimizer.step(closure=optimizer_closure)
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank35]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank35]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank35]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank35]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank35]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank35]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank35]:     return optimizer.step(closure=closure, **kwargs)
[rank35]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank35]:     out = func(*args, **kwargs)
[rank35]:           ^^^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank35]:     loss = closure()
[rank35]:            ^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank35]:     self._after_closure(model, optimizer)
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank35]:     self._clip_gradients(
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank35]:     call._call_lightning_module_hook(
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank35]:     output = fn(*args, **kwargs)
[rank35]:              ^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank35]:     self.clip_gradients(
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank35]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank35]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank35]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank35]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank35]:     return func(*args, **kwargs)
[rank35]:            ^^^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank35]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank35]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank35]:     return func(*args, **kwargs)
[rank35]:            ^^^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank35]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank35]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank35]:     return disable_fn(*args, **kwargs)
[rank35]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank35]:     return fn(*args, **kwargs)
[rank35]:            ^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank35]:     return DTensor._op_dispatcher.dispatch(
[rank35]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank35]:     self.sharding_propagator.propagate(op_info)
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank35]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank35]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank35]:     return self.cache(*args, **kwargs)
[rank35]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank35]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank35]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank35]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank35]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank35]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank35]:     raise ValueError(
[rank35]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63], mesh_dim_names=('data_parallel',)).
[rank41]: Traceback (most recent call last):
[rank41]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank41]:     main()
[rank41]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank41]:     llm.api.finetune(
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank41]:     return train(
[rank41]:            ^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank41]:     trainer.fit(model, data)
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank41]:     call._call_and_handle_interrupt(
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank41]:     return trainer_fn(*args, **kwargs)
[rank41]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank41]:     self._run(model, ckpt_path=ckpt_path)
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank41]:     results = self._run_stage()
[rank41]:               ^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank41]:     self.fit_loop.run()
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank41]:     self.advance()
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank41]:     self.epoch_loop.run(self._data_fetcher)
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank41]:     self.advance(data_fetcher)
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank41]:     super().advance(data_fetcher)
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank41]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank41]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank41]:     self._optimizer_step(batch_idx, closure)
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank41]:     call._call_lightning_module_hook(
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank41]:     output = fn(*args, **kwargs)
[rank41]:              ^^^^^^^^^^^^^^^^^^^
[rank1]: Traceback (most recent call last):
[rank1]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank1]:     main()
[rank1]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank1]:     llm.api.finetune(
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank1]:     return train(
[rank1]:            ^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank1]:     trainer.fit(model, data)
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank1]:     call._call_and_handle_interrupt(
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank41]:     optimizer.step(closure=optimizer_closure)
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank41]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank41]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank41]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank41]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank41]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank41]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank41]:     return optimizer.step(closure=closure, **kwargs)
[rank41]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank41]:     out = func(*args, **kwargs)
[rank41]:           ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank1]:     return trainer_fn(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank1]:     self._run(model, ckpt_path=ckpt_path)
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank1]:     results = self._run_stage()
[rank1]:               ^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank1]:     self.fit_loop.run()
[rank24]: Traceback (most recent call last):
[rank24]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank24]:     main()
[rank24]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank24]:     llm.api.finetune(
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank24]:     return train(
[rank24]:            ^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank24]:     trainer.fit(model, data)
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank24]:     call._call_and_handle_interrupt(
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank41]:     loss = closure()
[rank41]:            ^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank41]:     self._after_closure(model, optimizer)
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank41]:     self._clip_gradients(
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank41]:     call._call_lightning_module_hook(
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank1]:     self.advance()
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank1]:     self.epoch_loop.run(self._data_fetcher)
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank1]:     self.advance(data_fetcher)
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank1]:     super().advance(data_fetcher)
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank41]:     output = fn(*args, **kwargs)
[rank41]:              ^^^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank41]:     self.clip_gradients(
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank41]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank1]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank1]:     self._optimizer_step(batch_idx, closure)
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank1]:     call._call_lightning_module_hook(
[rank41]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank1]:     output = fn(*args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank41]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank41]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank41]:     return func(*args, **kwargs)
[rank41]:            ^^^^^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank1]:     optimizer.step(closure=optimizer_closure)
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank1]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank1]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank1]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank41]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank41]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank41]:     return func(*args, **kwargs)
[rank41]:            ^^^^^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank41]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank41]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank41]:     return disable_fn(*args, **kwargs)
[rank41]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank1]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank1]:     return optimizer.step(closure=closure, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank1]:     out = func(*args, **kwargs)
[rank1]:           ^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank24]:     return trainer_fn(*args, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank24]:     self._run(model, ckpt_path=ckpt_path)
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank24]:     results = self._run_stage()
[rank24]:               ^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank24]:     self.fit_loop.run()
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank41]:     return fn(*args, **kwargs)
[rank41]:            ^^^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank41]:     return DTensor._op_dispatcher.dispatch(
[rank41]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank41]:     self.sharding_propagator.propagate(op_info)
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank1]:     loss = closure()
[rank1]:            ^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank1]:     self._after_closure(model, optimizer)
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank1]:     self._clip_gradients(
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank1]:     call._call_lightning_module_hook(
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank24]:     self.advance()
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank24]:     self.epoch_loop.run(self._data_fetcher)
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank24]:     self.advance(data_fetcher)
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank24]:     super().advance(data_fetcher)
[rank41]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank41]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank41]:     return self.cache(*args, **kwargs)
[rank41]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank41]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank41]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank1]:     output = fn(*args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank1]:     self.clip_gradients(
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank1]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank24]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank24]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank24]:     self._optimizer_step(batch_idx, closure)
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank24]:     call._call_lightning_module_hook(
[rank41]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank41]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank24]:     output = fn(*args, **kwargs)
[rank24]:              ^^^^^^^^^^^^^^^^^^^
[rank41]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank41]:     raise ValueError(
[rank41]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61], mesh_dim_names=('data_parallel',)).
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank1]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank1]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank1]:     return func(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank24]:     optimizer.step(closure=optimizer_closure)
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank24]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank24]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank24]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank1]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank1]:     return func(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank1]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank1]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank1]:     return disable_fn(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank24]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank24]:     return optimizer.step(closure=closure, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank24]:     out = func(*args, **kwargs)
[rank24]:           ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank1]:     return DTensor._op_dispatcher.dispatch(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank1]:     self.sharding_propagator.propagate(op_info)
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank1]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank24]:     loss = closure()
[rank24]:            ^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank24]:     self._after_closure(model, optimizer)
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank24]:     self._clip_gradients(
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank24]:     call._call_lightning_module_hook(
[rank1]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank1]:     return self.cache(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank1]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank1]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank1]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank24]:     output = fn(*args, **kwargs)
[rank24]:              ^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank24]:     self.clip_gradients(
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank24]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank1]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank1]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank1]:     raise ValueError(
[rank1]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61], mesh_dim_names=('data_parallel',)).
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank24]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank24]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank24]:     return func(*args, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank24]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank24]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank24]:     return func(*args, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank24]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank24]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank24]:     return disable_fn(*args, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank24]:     return fn(*args, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank24]:     return DTensor._op_dispatcher.dispatch(
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank24]:     self.sharding_propagator.propagate(op_info)
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank24]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank24]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank24]:     return self.cache(*args, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank24]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank24]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank24]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank24]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]: Traceback (most recent call last):
[rank25]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank25]:     main()
[rank25]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank25]:     llm.api.finetune(
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank25]:     return train(
[rank25]:            ^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank25]:     trainer.fit(model, data)
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank25]:     call._call_and_handle_interrupt(
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank25]:     return trainer_fn(*args, **kwargs)
[rank25]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank25]:     self._run(model, ckpt_path=ckpt_path)
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank25]:     results = self._run_stage()
[rank25]:               ^^^^^^^^^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank25]:     self.fit_loop.run()
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank25]:     self.advance()
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank25]:     self.epoch_loop.run(self._data_fetcher)
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank25]:     self.advance(data_fetcher)
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank25]:     super().advance(data_fetcher)
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank25]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank25]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank25]:     self._optimizer_step(batch_idx, closure)
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank25]:     call._call_lightning_module_hook(
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank25]:     output = fn(*args, **kwargs)
[rank25]:              ^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank24]:     raise ValueError(
[rank24]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60], mesh_dim_names=('data_parallel',)).
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank25]:     optimizer.step(closure=optimizer_closure)
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank25]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank25]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank25]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank25]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]: Traceback (most recent call last):
[rank2]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank2]:     main()
[rank2]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank2]:     llm.api.finetune(
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank2]:     return train(
[rank2]:            ^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank2]:     trainer.fit(model, data)
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank2]:     call._call_and_handle_interrupt(
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank25]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank25]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank25]:     return optimizer.step(closure=closure, **kwargs)
[rank25]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank25]:     out = func(*args, **kwargs)
[rank25]:           ^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank25]:     loss = closure()
[rank25]:            ^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank25]:     self._after_closure(model, optimizer)
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank25]:     self._clip_gradients(
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank25]:     call._call_lightning_module_hook(
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank25]:     output = fn(*args, **kwargs)
[rank25]:              ^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank25]:     self.clip_gradients(
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank25]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank2]:     return trainer_fn(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank2]:     self._run(model, ckpt_path=ckpt_path)
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank2]:     results = self._run_stage()
[rank2]:               ^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank2]:     self.fit_loop.run()
[rank25]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank2]:     self.advance()
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank2]:     self.epoch_loop.run(self._data_fetcher)
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank2]:     self.advance(data_fetcher)
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank2]:     super().advance(data_fetcher)
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank25]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank25]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank25]:     return func(*args, **kwargs)
[rank25]:            ^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank2]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank2]:     self._optimizer_step(batch_idx, closure)
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank2]:     call._call_lightning_module_hook(
[rank25]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank25]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank25]:     return func(*args, **kwargs)
[rank25]:            ^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank25]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank25]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank25]:     return disable_fn(*args, **kwargs)
[rank25]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank2]:     output = fn(*args, **kwargs)
[rank2]:              ^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank25]:     return fn(*args, **kwargs)
[rank25]:            ^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank25]:     return DTensor._op_dispatcher.dispatch(
[rank25]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank25]:     self.sharding_propagator.propagate(op_info)
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank2]:     optimizer.step(closure=optimizer_closure)
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank2]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank2]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank2]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank25]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank25]:     return self.cache(*args, **kwargs)
[rank25]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank25]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank25]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank2]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank2]:     return optimizer.step(closure=closure, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank2]:     out = func(*args, **kwargs)
[rank2]:           ^^^^^^^^^^^^^^^^^^^^^
[rank25]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank25]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank2]:     loss = closure()
[rank2]:            ^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank2]:     self._after_closure(model, optimizer)
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank2]:     self._clip_gradients(
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank2]:     call._call_lightning_module_hook(
[rank25]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank25]:     raise ValueError(
[rank25]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61], mesh_dim_names=('data_parallel',)).
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank2]:     output = fn(*args, **kwargs)
[rank2]:              ^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank2]:     self.clip_gradients(
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank2]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank5]: Traceback (most recent call last):
[rank5]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank5]:     main()
[rank5]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank5]:     llm.api.finetune(
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank5]:     return train(
[rank5]:            ^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank5]:     trainer.fit(model, data)
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank5]:     call._call_and_handle_interrupt(
[rank2]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank5]:     return trainer_fn(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank5]:     self._run(model, ckpt_path=ckpt_path)
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank5]:     results = self._run_stage()
[rank5]:               ^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank5]:     self.fit_loop.run()
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank2]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank2]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank2]:     return func(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank2]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank2]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank2]:     return func(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank2]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank2]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank2]:     return disable_fn(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank2]:     return DTensor._op_dispatcher.dispatch(
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank2]:     self.sharding_propagator.propagate(op_info)
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank2]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank2]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank2]:     return self.cache(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank2]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank2]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank2]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank2]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank5]:     self.advance()
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank5]:     self.epoch_loop.run(self._data_fetcher)
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank5]:     self.advance(data_fetcher)
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank5]:     super().advance(data_fetcher)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank3]:     main()
[rank3]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank3]:     llm.api.finetune(
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank3]:     return train(
[rank3]:            ^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank3]:     trainer.fit(model, data)
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank3]:     call._call_and_handle_interrupt(
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank5]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank5]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank5]:     self._optimizer_step(batch_idx, closure)
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank5]:     call._call_lightning_module_hook(
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank3]:     return trainer_fn(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank3]:     self._run(model, ckpt_path=ckpt_path)
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank3]:     results = self._run_stage()
[rank3]:               ^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank3]:     self.fit_loop.run()
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank5]:     output = fn(*args, **kwargs)
[rank5]:              ^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank3]:     self.advance()
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank3]:     self.epoch_loop.run(self._data_fetcher)
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank3]:     self.advance(data_fetcher)
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank3]:     super().advance(data_fetcher)
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank5]:     optimizer.step(closure=optimizer_closure)
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank5]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank5]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank5]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank3]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank3]:     self._optimizer_step(batch_idx, closure)
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank3]:     call._call_lightning_module_hook(
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank5]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank5]:     return optimizer.step(closure=closure, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank5]:     out = func(*args, **kwargs)
[rank5]:           ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank3]:     output = fn(*args, **kwargs)
[rank3]:              ^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank5]:     loss = closure()
[rank5]:            ^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank5]:     self._after_closure(model, optimizer)
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank5]:     self._clip_gradients(
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank5]:     call._call_lightning_module_hook(
[rank2]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank2]:     raise ValueError(
[rank2]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62], mesh_dim_names=('data_parallel',)).
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank5]:     output = fn(*args, **kwargs)
[rank5]:              ^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank5]:     self.clip_gradients(
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank5]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank3]:     optimizer.step(closure=optimizer_closure)
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank3]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank3]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank3]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank3]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank3]:     return optimizer.step(closure=closure, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank3]:     out = func(*args, **kwargs)
[rank3]:           ^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank5]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank5]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank5]:     return func(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank3]:     loss = closure()
[rank3]:            ^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank3]:     self._after_closure(model, optimizer)
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank3]:     self._clip_gradients(
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank3]:     call._call_lightning_module_hook(
[rank5]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank5]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank5]:     return func(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank5]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank5]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank5]:     return disable_fn(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank3]:     output = fn(*args, **kwargs)
[rank3]:              ^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank3]:     self.clip_gradients(
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank3]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank5]:     return fn(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank5]:     return DTensor._op_dispatcher.dispatch(
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank5]:     self.sharding_propagator.propagate(op_info)
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank5]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank3]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank5]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank5]:     return self.cache(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank5]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank5]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank5]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank3]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank3]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank3]:     return func(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank5]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank3]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank3]:     return func(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank3]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank3]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank3]:     return disable_fn(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank5]:     raise ValueError(
[rank5]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61], mesh_dim_names=('data_parallel',)).
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank3]:     return fn(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank3]:     return DTensor._op_dispatcher.dispatch(
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank3]:     self.sharding_propagator.propagate(op_info)
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank3]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank3]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank3]:     return self.cache(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank3]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank3]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank3]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank3]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank3]:     raise ValueError(
[rank3]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63], mesh_dim_names=('data_parallel',)).
[rank0]: Traceback (most recent call last):
[rank0]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank0]:     main()
[rank0]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank0]:     llm.api.finetune(
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank0]:     return train(
[rank0]:            ^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank0]:     trainer.fit(model, data)
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank0]:     call._call_and_handle_interrupt(
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank0]:     return trainer_fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank0]:     self._run(model, ckpt_path=ckpt_path)
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank0]:     results = self._run_stage()
[rank0]:               ^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank0]:     self.fit_loop.run()
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank0]:     self.advance()
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank0]:     self.epoch_loop.run(self._data_fetcher)
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank0]:     self.advance(data_fetcher)
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank0]:     super().advance(data_fetcher)
[rank7]: Traceback (most recent call last):
[rank7]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank7]:     main()
[rank7]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank7]:     llm.api.finetune(
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank7]:     return train(
[rank7]:            ^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank7]:     trainer.fit(model, data)
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank7]:     call._call_and_handle_interrupt(
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank0]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank0]:     self._optimizer_step(batch_idx, closure)
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank0]:     call._call_lightning_module_hook(
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank7]:     return trainer_fn(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank7]:     self._run(model, ckpt_path=ckpt_path)
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank7]:     results = self._run_stage()
[rank7]:               ^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank7]:     self.fit_loop.run()
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank0]:     output = fn(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank0]:     optimizer.step(closure=optimizer_closure)
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank0]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank0]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank0]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank0]:     return optimizer.step(closure=closure, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank0]:     loss = closure()
[rank0]:            ^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank0]:     self._after_closure(model, optimizer)
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank0]:     self._clip_gradients(
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank0]:     call._call_lightning_module_hook(
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank0]:     output = fn(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank0]:     self.clip_gradients(
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank0]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank7]:     self.advance()
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank7]:     self.epoch_loop.run(self._data_fetcher)
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank7]:     self.advance(data_fetcher)
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank7]:     super().advance(data_fetcher)
[rank0]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank7]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank7]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank7]:     self._optimizer_step(batch_idx, closure)
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank7]:     call._call_lightning_module_hook(
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank0]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank0]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank7]:     output = fn(*args, **kwargs)
[rank7]:              ^^^^^^^^^^^^^^^^^^^
[rank0]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank0]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank0]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank0]:     return disable_fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank7]:     optimizer.step(closure=optimizer_closure)
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank7]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank7]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank7]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank0]:     return DTensor._op_dispatcher.dispatch(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank0]:     self.sharding_propagator.propagate(op_info)
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank0]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank7]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank7]:     return optimizer.step(closure=closure, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank7]:     out = func(*args, **kwargs)
[rank7]:           ^^^^^^^^^^^^^^^^^^^^^
[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank0]:     return self.cache(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank0]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank0]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank7]:     loss = closure()
[rank7]:            ^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank7]:     self._after_closure(model, optimizer)
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank7]:     self._clip_gradients(
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank7]:     call._call_lightning_module_hook(
[rank40]: Traceback (most recent call last):
[rank40]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank40]:     main()
[rank40]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank40]:     llm.api.finetune(
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank40]:     return train(
[rank40]:            ^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank40]:     trainer.fit(model, data)
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank40]:     call._call_and_handle_interrupt(
[rank0]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank7]:     output = fn(*args, **kwargs)
[rank7]:              ^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank7]:     self.clip_gradients(
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank7]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank40]:     return trainer_fn(*args, **kwargs)
[rank40]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank40]:     self._run(model, ckpt_path=ckpt_path)
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank40]:     results = self._run_stage()
[rank40]:               ^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank40]:     self.fit_loop.run()
[rank0]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank0]:     raise ValueError(
[rank0]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60], mesh_dim_names=('data_parallel',)).
[rank7]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank40]:     self.advance()
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank40]:     self.epoch_loop.run(self._data_fetcher)
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank40]:     self.advance(data_fetcher)
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank40]:     super().advance(data_fetcher)
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank7]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank7]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank7]:     return func(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank40]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank40]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank40]:     self._optimizer_step(batch_idx, closure)
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank40]:     call._call_lightning_module_hook(
[rank7]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank7]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank7]:     return func(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank7]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank7]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank7]:     return disable_fn(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank40]:     output = fn(*args, **kwargs)
[rank40]:              ^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank7]:     return fn(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank7]:     return DTensor._op_dispatcher.dispatch(
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank7]:     self.sharding_propagator.propagate(op_info)
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank7]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank40]:     optimizer.step(closure=optimizer_closure)
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank40]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank40]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank40]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank40]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank7]:     return self.cache(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank7]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank7]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank7]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank40]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank40]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank40]:     return optimizer.step(closure=closure, **kwargs)
[rank40]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank40]:     out = func(*args, **kwargs)
[rank40]:           ^^^^^^^^^^^^^^^^^^^^^
[rank7]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank40]:     loss = closure()
[rank40]:            ^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank40]:     self._after_closure(model, optimizer)
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank40]:     self._clip_gradients(
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank40]:     call._call_lightning_module_hook(
[rank7]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank7]:     raise ValueError(
[rank7]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63], mesh_dim_names=('data_parallel',)).
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank40]:     output = fn(*args, **kwargs)
[rank40]:              ^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank40]:     self.clip_gradients(
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank40]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank40]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank40]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank40]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank40]:     return func(*args, **kwargs)
[rank40]:            ^^^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank40]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank40]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank40]:     return func(*args, **kwargs)
[rank40]:            ^^^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank40]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank40]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank40]:     return disable_fn(*args, **kwargs)
[rank40]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank40]:     return fn(*args, **kwargs)
[rank40]:            ^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank40]:     return DTensor._op_dispatcher.dispatch(
[rank40]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank40]:     self.sharding_propagator.propagate(op_info)
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank40]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank40]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank40]:     return self.cache(*args, **kwargs)
[rank40]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank40]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank40]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank40]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank40]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank40]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank40]:     raise ValueError(
[rank40]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60], mesh_dim_names=('data_parallel',)).
[rank27]: Traceback (most recent call last):
[rank27]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank27]:     main()
[rank27]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank27]:     llm.api.finetune(
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank27]:     return train(
[rank27]:            ^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank27]:     trainer.fit(model, data)
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank27]:     call._call_and_handle_interrupt(
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank27]:     return trainer_fn(*args, **kwargs)
[rank27]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank27]:     self._run(model, ckpt_path=ckpt_path)
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank27]:     results = self._run_stage()
[rank27]:               ^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank27]:     self.fit_loop.run()
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank27]:     self.advance()
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank27]:     self.epoch_loop.run(self._data_fetcher)
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank27]:     self.advance(data_fetcher)
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank27]:     super().advance(data_fetcher)
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank27]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank27]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank27]:     self._optimizer_step(batch_idx, closure)
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank27]:     call._call_lightning_module_hook(
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank27]:     output = fn(*args, **kwargs)
[rank27]:              ^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank27]:     optimizer.step(closure=optimizer_closure)
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank27]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank27]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank27]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank27]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank27]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank27]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank27]:     return optimizer.step(closure=closure, **kwargs)
[rank27]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank27]:     out = func(*args, **kwargs)
[rank27]:           ^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank27]:     loss = closure()
[rank27]:            ^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank27]:     self._after_closure(model, optimizer)
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank27]:     self._clip_gradients(
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank27]:     call._call_lightning_module_hook(
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank27]:     output = fn(*args, **kwargs)
[rank27]:              ^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank27]:     self.clip_gradients(
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank27]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank27]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank27]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank27]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank27]:     return func(*args, **kwargs)
[rank27]:            ^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank27]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank27]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank27]:     return func(*args, **kwargs)
[rank27]:            ^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank27]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank27]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank27]:     return disable_fn(*args, **kwargs)
[rank27]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank27]:     return fn(*args, **kwargs)
[rank27]:            ^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank27]:     return DTensor._op_dispatcher.dispatch(
[rank27]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank27]:     self.sharding_propagator.propagate(op_info)
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank27]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank27]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank27]:     return self.cache(*args, **kwargs)
[rank27]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank27]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank27]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank27]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank27]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank27]:     raise ValueError(
[rank27]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63], mesh_dim_names=('data_parallel',)).
[rank26]: Traceback (most recent call last):
[rank26]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank26]:     main()
[rank26]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank26]:     llm.api.finetune(
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank26]:     return train(
[rank26]:            ^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank26]:     trainer.fit(model, data)
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank26]:     call._call_and_handle_interrupt(
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank26]:     return trainer_fn(*args, **kwargs)
[rank26]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank26]:     self._run(model, ckpt_path=ckpt_path)
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank26]:     results = self._run_stage()
[rank26]:               ^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank26]:     self.fit_loop.run()
[rank31]: Traceback (most recent call last):
[rank31]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank31]:     main()
[rank31]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank31]:     llm.api.finetune(
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank31]:     return train(
[rank31]:            ^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank31]:     trainer.fit(model, data)
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank31]:     call._call_and_handle_interrupt(
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank26]:     self.advance()
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank26]:     self.epoch_loop.run(self._data_fetcher)
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank26]:     self.advance(data_fetcher)
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank26]:     super().advance(data_fetcher)
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank26]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank26]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank26]:     self._optimizer_step(batch_idx, closure)
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank26]:     call._call_lightning_module_hook(
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank26]:     output = fn(*args, **kwargs)
[rank26]:              ^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank26]:     optimizer.step(closure=optimizer_closure)
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank26]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank26]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank26]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank26]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank26]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank26]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank26]:     return optimizer.step(closure=closure, **kwargs)
[rank26]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank26]:     out = func(*args, **kwargs)
[rank26]:           ^^^^^^^^^^^^^^^^^^^^^
[rank4]: Traceback (most recent call last):
[rank4]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank4]:     main()
[rank4]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank4]:     llm.api.finetune(
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank4]:     return train(
[rank4]:            ^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank4]:     trainer.fit(model, data)
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank4]:     call._call_and_handle_interrupt(
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank31]:     return trainer_fn(*args, **kwargs)
[rank31]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank31]:     self._run(model, ckpt_path=ckpt_path)
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank31]:     results = self._run_stage()
[rank31]:               ^^^^^^^^^^^^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank31]:     self.fit_loop.run()
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank26]:     loss = closure()
[rank26]:            ^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank26]:     self._after_closure(model, optimizer)
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank26]:     self._clip_gradients(
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank26]:     call._call_lightning_module_hook(
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank31]:     self.advance()
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank31]:     self.epoch_loop.run(self._data_fetcher)
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank31]:     self.advance(data_fetcher)
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank31]:     super().advance(data_fetcher)
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank26]:     output = fn(*args, **kwargs)
[rank26]:              ^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank26]:     self.clip_gradients(
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank26]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank31]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank31]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank31]:     self._optimizer_step(batch_idx, closure)
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank31]:     call._call_lightning_module_hook(
[rank26]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank31]:     output = fn(*args, **kwargs)
[rank31]:              ^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank26]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank26]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank26]:     return func(*args, **kwargs)
[rank26]:            ^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank31]:     optimizer.step(closure=optimizer_closure)
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank31]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank31]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank31]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank31]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank26]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank26]:     return func(*args, **kwargs)
[rank26]:            ^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank26]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank26]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank26]:     return disable_fn(*args, **kwargs)
[rank26]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank4]:     return trainer_fn(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank4]:     self._run(model, ckpt_path=ckpt_path)
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank4]:     results = self._run_stage()
[rank4]:               ^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank4]:     self.fit_loop.run()
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank31]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank31]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank31]:     return optimizer.step(closure=closure, **kwargs)
[rank31]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank31]:     out = func(*args, **kwargs)
[rank31]:           ^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank26]:     return fn(*args, **kwargs)
[rank26]:            ^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank26]:     return DTensor._op_dispatcher.dispatch(
[rank26]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank26]:     self.sharding_propagator.propagate(op_info)
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank4]:     self.advance()
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank4]:     self.epoch_loop.run(self._data_fetcher)
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank4]:     self.advance(data_fetcher)
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank4]:     super().advance(data_fetcher)
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank31]:     loss = closure()
[rank31]:            ^^^^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank31]:     self._after_closure(model, optimizer)
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank31]:     self._clip_gradients(
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank31]:     call._call_lightning_module_hook(
[rank26]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank26]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank26]:     return self.cache(*args, **kwargs)
[rank26]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank26]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank26]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank4]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank4]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank4]:     self._optimizer_step(batch_idx, closure)
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank4]:     call._call_lightning_module_hook(
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank31]:     output = fn(*args, **kwargs)
[rank31]:              ^^^^^^^^^^^^^^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank31]:     self.clip_gradients(
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank31]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank26]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank26]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank4]:     output = fn(*args, **kwargs)
[rank4]:              ^^^^^^^^^^^^^^^^^^^
[rank31]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank26]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank26]:     raise ValueError(
[rank26]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62], mesh_dim_names=('data_parallel',)).
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank4]:     optimizer.step(closure=optimizer_closure)
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank4]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank4]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank4]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank31]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank31]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank31]:     return func(*args, **kwargs)
[rank31]:            ^^^^^^^^^^^^^^^^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank4]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank4]:     return optimizer.step(closure=closure, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank4]:     out = func(*args, **kwargs)
[rank4]:           ^^^^^^^^^^^^^^^^^^^^^
[rank31]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank31]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank31]:     return func(*args, **kwargs)
[rank31]:            ^^^^^^^^^^^^^^^^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank31]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank31]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank31]:     return disable_fn(*args, **kwargs)
[rank31]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank4]:     loss = closure()
[rank4]:            ^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank4]:     self._after_closure(model, optimizer)
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank4]:     self._clip_gradients(
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank4]:     call._call_lightning_module_hook(
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank31]:     return fn(*args, **kwargs)
[rank31]:            ^^^^^^^^^^^^^^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank31]:     return DTensor._op_dispatcher.dispatch(
[rank31]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank31]:     self.sharding_propagator.propagate(op_info)
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank4]:     output = fn(*args, **kwargs)
[rank4]:              ^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank4]:     self.clip_gradients(
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank4]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank31]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank31]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank31]:     return self.cache(*args, **kwargs)
[rank31]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank31]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank31]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank4]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank31]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank31]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank4]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank4]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank4]:     return func(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank31]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank31]:     raise ValueError(
[rank31]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63], mesh_dim_names=('data_parallel',)).
[rank4]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank4]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank4]:     return func(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank4]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank4]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank4]:     return disable_fn(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank4]:     return fn(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank4]:     return DTensor._op_dispatcher.dispatch(
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank4]:     self.sharding_propagator.propagate(op_info)
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank4]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank4]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank4]:     return self.cache(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank4]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank4]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank4]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank4]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank4]:     raise ValueError(
[rank4]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60], mesh_dim_names=('data_parallel',)).
[rank39]: Traceback (most recent call last):
[rank39]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank39]:     main()
[rank39]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank39]:     llm.api.finetune(
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank39]:     return train(
[rank39]:            ^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank39]:     trainer.fit(model, data)
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank39]:     call._call_and_handle_interrupt(
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank39]:     return trainer_fn(*args, **kwargs)
[rank39]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank39]:     self._run(model, ckpt_path=ckpt_path)
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank39]:     results = self._run_stage()
[rank39]:               ^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank39]:     self.fit_loop.run()
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank39]:     self.advance()
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank39]:     self.epoch_loop.run(self._data_fetcher)
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank39]:     self.advance(data_fetcher)
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank39]:     super().advance(data_fetcher)
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank39]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank39]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank39]:     self._optimizer_step(batch_idx, closure)
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank39]:     call._call_lightning_module_hook(
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank39]:     output = fn(*args, **kwargs)
[rank39]:              ^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank39]:     optimizer.step(closure=optimizer_closure)
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank39]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank39]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank39]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank39]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank39]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank39]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank39]:     return optimizer.step(closure=closure, **kwargs)
[rank39]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank39]:     out = func(*args, **kwargs)
[rank39]:           ^^^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank39]:     loss = closure()
[rank39]:            ^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank39]:     self._after_closure(model, optimizer)
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank39]:     self._clip_gradients(
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank39]:     call._call_lightning_module_hook(
[rank17]: Traceback (most recent call last):
[rank17]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank17]:     main()
[rank17]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank17]:     llm.api.finetune(
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank17]:     return train(
[rank17]:            ^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank17]:     trainer.fit(model, data)
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank17]:     call._call_and_handle_interrupt(
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank39]:     output = fn(*args, **kwargs)
[rank39]:              ^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank39]:     self.clip_gradients(
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank39]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank17]:     return trainer_fn(*args, **kwargs)
[rank17]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank17]:     self._run(model, ckpt_path=ckpt_path)
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank17]:     results = self._run_stage()
[rank17]:               ^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank17]:     self.fit_loop.run()
[rank39]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank17]:     self.advance()
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank17]:     self.epoch_loop.run(self._data_fetcher)
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank17]:     self.advance(data_fetcher)
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank17]:     super().advance(data_fetcher)
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank39]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank39]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank39]:     return func(*args, **kwargs)
[rank39]:            ^^^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank17]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank17]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank17]:     self._optimizer_step(batch_idx, closure)
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank17]:     call._call_lightning_module_hook(
[rank39]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank39]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank39]:     return func(*args, **kwargs)
[rank39]:            ^^^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank39]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank39]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank39]:     return disable_fn(*args, **kwargs)
[rank39]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank39]:     return fn(*args, **kwargs)
[rank39]:            ^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank39]:     return DTensor._op_dispatcher.dispatch(
[rank39]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank39]:     self.sharding_propagator.propagate(op_info)
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank39]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank39]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank39]:     return self.cache(*args, **kwargs)
[rank39]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank39]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank39]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank17]:     output = fn(*args, **kwargs)
[rank17]:              ^^^^^^^^^^^^^^^^^^^
[rank39]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank39]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank17]:     optimizer.step(closure=optimizer_closure)
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank17]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank17]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank17]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank17]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank39]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank39]:     raise ValueError(
[rank39]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63], mesh_dim_names=('data_parallel',)).
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank17]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank17]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank17]:     return optimizer.step(closure=closure, **kwargs)
[rank17]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank17]:     out = func(*args, **kwargs)
[rank17]:           ^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank17]:     loss = closure()
[rank17]:            ^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank17]:     self._after_closure(model, optimizer)
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank17]:     self._clip_gradients(
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank17]:     call._call_lightning_module_hook(
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank17]:     output = fn(*args, **kwargs)
[rank17]:              ^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank17]:     self.clip_gradients(
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank17]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank17]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank17]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank17]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank17]:     return func(*args, **kwargs)
[rank17]:            ^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank17]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank17]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank17]:     return func(*args, **kwargs)
[rank17]:            ^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank17]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank17]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank17]:     return disable_fn(*args, **kwargs)
[rank17]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank17]:     return fn(*args, **kwargs)
[rank17]:            ^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank17]:     return DTensor._op_dispatcher.dispatch(
[rank17]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank17]:     self.sharding_propagator.propagate(op_info)
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank17]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank17]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank17]:     return self.cache(*args, **kwargs)
[rank17]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank17]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank17]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank17]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank17]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank17]:     raise ValueError(
[rank17]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61], mesh_dim_names=('data_parallel',)).
[rank10]: Traceback (most recent call last):
[rank10]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank10]:     main()
[rank10]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank10]:     llm.api.finetune(
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank10]:     return train(
[rank10]:            ^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank10]:     trainer.fit(model, data)
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank10]:     call._call_and_handle_interrupt(
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank10]:     return trainer_fn(*args, **kwargs)
[rank10]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank10]:     self._run(model, ckpt_path=ckpt_path)
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank10]:     results = self._run_stage()
[rank10]:               ^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank10]:     self.fit_loop.run()
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank10]:     self.advance()
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank10]:     self.epoch_loop.run(self._data_fetcher)
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank10]:     self.advance(data_fetcher)
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank10]:     super().advance(data_fetcher)
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank10]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank10]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank10]:     self._optimizer_step(batch_idx, closure)
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank10]:     call._call_lightning_module_hook(
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank10]:     output = fn(*args, **kwargs)
[rank10]:              ^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank10]:     optimizer.step(closure=optimizer_closure)
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank10]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank10]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank10]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank10]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank10]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank10]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank10]:     return optimizer.step(closure=closure, **kwargs)
[rank10]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank10]:     out = func(*args, **kwargs)
[rank10]:           ^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank10]:     loss = closure()
[rank10]:            ^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank10]:     self._after_closure(model, optimizer)
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank10]:     self._clip_gradients(
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank10]:     call._call_lightning_module_hook(
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank10]:     output = fn(*args, **kwargs)
[rank10]:              ^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank10]:     self.clip_gradients(
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank10]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank10]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank10]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank10]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank10]:     return func(*args, **kwargs)
[rank10]:            ^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank10]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank10]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank10]:     return func(*args, **kwargs)
[rank10]:            ^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank10]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank10]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank10]:     return disable_fn(*args, **kwargs)
[rank10]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank10]:     return fn(*args, **kwargs)
[rank10]:            ^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank10]:     return DTensor._op_dispatcher.dispatch(
[rank10]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank10]:     self.sharding_propagator.propagate(op_info)
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank10]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank10]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank10]:     return self.cache(*args, **kwargs)
[rank10]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank10]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank10]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank10]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank10]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]: Traceback (most recent call last):
[rank16]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank16]:     main()
[rank16]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank16]:     llm.api.finetune(
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank16]:     return train(
[rank16]:            ^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank16]:     trainer.fit(model, data)
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank16]:     call._call_and_handle_interrupt(
[rank10]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank10]:     raise ValueError(
[rank10]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62], mesh_dim_names=('data_parallel',)).
[rank9]: Traceback (most recent call last):
[rank9]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank9]:     main()
[rank9]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank9]:     llm.api.finetune(
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank9]:     return train(
[rank9]:            ^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank9]:     trainer.fit(model, data)
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank9]:     call._call_and_handle_interrupt(
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank9]:     return trainer_fn(*args, **kwargs)
[rank9]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank9]:     self._run(model, ckpt_path=ckpt_path)
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank9]:     results = self._run_stage()
[rank9]:               ^^^^^^^^^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank9]:     self.fit_loop.run()
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank9]:     self.advance()
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank9]:     self.epoch_loop.run(self._data_fetcher)
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank9]:     self.advance(data_fetcher)
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank9]:     super().advance(data_fetcher)
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank9]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank9]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank9]:     self._optimizer_step(batch_idx, closure)
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank9]:     call._call_lightning_module_hook(
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank9]:     output = fn(*args, **kwargs)
[rank9]:              ^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank9]:     optimizer.step(closure=optimizer_closure)
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank9]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank9]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank9]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank9]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank16]:     return trainer_fn(*args, **kwargs)
[rank16]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank16]:     self._run(model, ckpt_path=ckpt_path)
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank16]:     results = self._run_stage()
[rank16]:               ^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank16]:     self.fit_loop.run()
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank9]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank9]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank9]:     return optimizer.step(closure=closure, **kwargs)
[rank9]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank9]:     out = func(*args, **kwargs)
[rank9]:           ^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank16]:     self.advance()
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank16]:     self.epoch_loop.run(self._data_fetcher)
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank16]:     self.advance(data_fetcher)
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank16]:     super().advance(data_fetcher)
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank9]:     loss = closure()
[rank9]:            ^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank9]:     self._after_closure(model, optimizer)
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank9]:     self._clip_gradients(
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank9]:     call._call_lightning_module_hook(
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank16]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank16]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank16]:     self._optimizer_step(batch_idx, closure)
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank16]:     call._call_lightning_module_hook(
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank9]:     output = fn(*args, **kwargs)
[rank9]:              ^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank9]:     self.clip_gradients(
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank9]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank16]:     output = fn(*args, **kwargs)
[rank16]:              ^^^^^^^^^^^^^^^^^^^
[rank9]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank16]:     optimizer.step(closure=optimizer_closure)
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank16]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank16]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank16]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank16]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank9]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank9]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank9]:     return func(*args, **kwargs)
[rank9]:            ^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank37]: Traceback (most recent call last):
[rank37]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank37]:     main()
[rank37]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank37]:     llm.api.finetune(
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank37]:     return train(
[rank37]:            ^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank37]:     trainer.fit(model, data)
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank37]:     call._call_and_handle_interrupt(
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank16]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank16]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank16]:     return optimizer.step(closure=closure, **kwargs)
[rank16]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank16]:     out = func(*args, **kwargs)
[rank16]:           ^^^^^^^^^^^^^^^^^^^^^
[rank9]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank9]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank9]:     return func(*args, **kwargs)
[rank9]:            ^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank9]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank9]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank9]:     return disable_fn(*args, **kwargs)
[rank9]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank16]:     loss = closure()
[rank16]:            ^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank16]:     self._after_closure(model, optimizer)
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank16]:     self._clip_gradients(
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank16]:     call._call_lightning_module_hook(
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank9]:     return fn(*args, **kwargs)
[rank9]:            ^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank9]:     return DTensor._op_dispatcher.dispatch(
[rank9]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank9]:     self.sharding_propagator.propagate(op_info)
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank9]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank16]:     output = fn(*args, **kwargs)
[rank16]:              ^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank16]:     self.clip_gradients(
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank16]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank9]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank9]:     return self.cache(*args, **kwargs)
[rank9]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank9]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank9]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank9]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank16]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank9]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank16]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank16]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank16]:     return func(*args, **kwargs)
[rank16]:            ^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank9]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank9]:     raise ValueError(
[rank9]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61], mesh_dim_names=('data_parallel',)).
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank37]:     return trainer_fn(*args, **kwargs)
[rank37]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank37]:     self._run(model, ckpt_path=ckpt_path)
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank37]:     results = self._run_stage()
[rank37]:               ^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank37]:     self.fit_loop.run()
[rank16]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank16]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank16]:     return func(*args, **kwargs)
[rank16]:            ^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank16]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank16]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank16]:     return disable_fn(*args, **kwargs)
[rank16]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]: Traceback (most recent call last):
[rank8]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank8]:     main()
[rank8]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank8]:     llm.api.finetune(
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank8]:     return train(
[rank8]:            ^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank8]:     trainer.fit(model, data)
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank8]:     call._call_and_handle_interrupt(
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank37]:     self.advance()
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank37]:     self.epoch_loop.run(self._data_fetcher)
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank37]:     self.advance(data_fetcher)
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank37]:     super().advance(data_fetcher)
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank16]:     return fn(*args, **kwargs)
[rank16]:            ^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank16]:     return DTensor._op_dispatcher.dispatch(
[rank16]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank16]:     self.sharding_propagator.propagate(op_info)
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank8]:     return trainer_fn(*args, **kwargs)
[rank8]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank8]:     self._run(model, ckpt_path=ckpt_path)
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank8]:     results = self._run_stage()
[rank8]:               ^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank8]:     self.fit_loop.run()
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank37]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank37]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank37]:     self._optimizer_step(batch_idx, closure)
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank37]:     call._call_lightning_module_hook(
[rank16]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank16]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank16]:     return self.cache(*args, **kwargs)
[rank16]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank16]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank16]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank8]:     self.advance()
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank8]:     self.epoch_loop.run(self._data_fetcher)
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank8]:     self.advance(data_fetcher)
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank8]:     super().advance(data_fetcher)
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank37]:     output = fn(*args, **kwargs)
[rank37]:              ^^^^^^^^^^^^^^^^^^^
[rank16]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank16]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank8]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank8]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank8]:     self._optimizer_step(batch_idx, closure)
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank8]:     call._call_lightning_module_hook(
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank37]:     optimizer.step(closure=optimizer_closure)
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank37]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank37]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank37]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank37]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank16]:     raise ValueError(
[rank16]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60], mesh_dim_names=('data_parallel',)).
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank8]:     output = fn(*args, **kwargs)
[rank8]:              ^^^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank37]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank37]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank37]:     return optimizer.step(closure=closure, **kwargs)
[rank37]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank37]:     out = func(*args, **kwargs)
[rank37]:           ^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank8]:     optimizer.step(closure=optimizer_closure)
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank8]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank8]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank8]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank8]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank37]:     loss = closure()
[rank37]:            ^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank37]:     self._after_closure(model, optimizer)
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank37]:     self._clip_gradients(
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank37]:     call._call_lightning_module_hook(
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank8]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank8]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank8]:     return optimizer.step(closure=closure, **kwargs)
[rank8]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank8]:     out = func(*args, **kwargs)
[rank8]:           ^^^^^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank37]:     output = fn(*args, **kwargs)
[rank37]:              ^^^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank37]:     self.clip_gradients(
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank37]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank8]:     loss = closure()
[rank8]:            ^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank8]:     self._after_closure(model, optimizer)
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank8]:     self._clip_gradients(
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank8]:     call._call_lightning_module_hook(
[rank37]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank8]:     output = fn(*args, **kwargs)
[rank8]:              ^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank8]:     self.clip_gradients(
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank8]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank37]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank37]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank37]:     return func(*args, **kwargs)
[rank37]:            ^^^^^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank8]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank37]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank37]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank37]:     return func(*args, **kwargs)
[rank37]:            ^^^^^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank37]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank37]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank37]:     return disable_fn(*args, **kwargs)
[rank37]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank8]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank8]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank8]:     return func(*args, **kwargs)
[rank8]:            ^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank37]:     return fn(*args, **kwargs)
[rank37]:            ^^^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank37]:     return DTensor._op_dispatcher.dispatch(
[rank37]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank37]:     self.sharding_propagator.propagate(op_info)
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank8]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank8]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank8]:     return func(*args, **kwargs)
[rank8]:            ^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank8]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank8]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank8]:     return disable_fn(*args, **kwargs)
[rank8]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank37]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank37]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank37]:     return self.cache(*args, **kwargs)
[rank37]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank37]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank37]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank8]:     return fn(*args, **kwargs)
[rank8]:            ^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank8]:     return DTensor._op_dispatcher.dispatch(
[rank8]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank8]:     self.sharding_propagator.propagate(op_info)
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank8]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank37]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank37]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank8]:     return self.cache(*args, **kwargs)
[rank8]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank8]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank8]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank8]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank37]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank37]:     raise ValueError(
[rank37]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61], mesh_dim_names=('data_parallel',)).
[rank8]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank30]: Traceback (most recent call last):
[rank30]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank30]:     main()
[rank30]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank30]:     llm.api.finetune(
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank30]:     return train(
[rank30]:            ^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank30]:     trainer.fit(model, data)
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank30]:     call._call_and_handle_interrupt(
[rank8]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank8]:     raise ValueError(
[rank8]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60], mesh_dim_names=('data_parallel',)).
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank30]:     return trainer_fn(*args, **kwargs)
[rank30]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank30]:     self._run(model, ckpt_path=ckpt_path)
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank30]:     results = self._run_stage()
[rank30]:               ^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank30]:     self.fit_loop.run()
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank30]:     self.advance()
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank30]:     self.epoch_loop.run(self._data_fetcher)
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank30]:     self.advance(data_fetcher)
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank30]:     super().advance(data_fetcher)
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank30]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank30]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank30]:     self._optimizer_step(batch_idx, closure)
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank30]:     call._call_lightning_module_hook(
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank30]:     output = fn(*args, **kwargs)
[rank30]:              ^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank30]:     optimizer.step(closure=optimizer_closure)
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank30]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank30]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank30]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank30]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank30]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank30]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank30]:     return optimizer.step(closure=closure, **kwargs)
[rank30]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank30]:     out = func(*args, **kwargs)
[rank30]:           ^^^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank30]:     loss = closure()
[rank30]:            ^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank30]:     self._after_closure(model, optimizer)
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank30]:     self._clip_gradients(
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank30]:     call._call_lightning_module_hook(
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank30]:     output = fn(*args, **kwargs)
[rank30]:              ^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank30]:     self.clip_gradients(
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank30]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank30]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank30]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank30]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank30]:     return func(*args, **kwargs)
[rank30]:            ^^^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank30]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank30]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank30]:     return func(*args, **kwargs)
[rank30]:            ^^^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank30]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank30]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank30]:     return disable_fn(*args, **kwargs)
[rank30]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank30]:     return fn(*args, **kwargs)
[rank30]:            ^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank30]:     return DTensor._op_dispatcher.dispatch(
[rank30]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank30]:     self.sharding_propagator.propagate(op_info)
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank30]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank30]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank30]:     return self.cache(*args, **kwargs)
[rank30]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank30]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank30]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank30]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank30]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank30]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank30]:     raise ValueError(
[rank30]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62], mesh_dim_names=('data_parallel',)).
[rank11]: Traceback (most recent call last):
[rank11]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank11]:     main()
[rank11]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank11]:     llm.api.finetune(
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank11]:     return train(
[rank11]:            ^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank11]:     trainer.fit(model, data)
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank11]:     call._call_and_handle_interrupt(
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank11]:     return trainer_fn(*args, **kwargs)
[rank11]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank11]:     self._run(model, ckpt_path=ckpt_path)
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank11]:     results = self._run_stage()
[rank11]:               ^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank11]:     self.fit_loop.run()
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank11]:     self.advance()
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank11]:     self.epoch_loop.run(self._data_fetcher)
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank11]:     self.advance(data_fetcher)
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank11]:     super().advance(data_fetcher)
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank11]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank11]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank11]:     self._optimizer_step(batch_idx, closure)
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank11]:     call._call_lightning_module_hook(
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank11]:     output = fn(*args, **kwargs)
[rank11]:              ^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank11]:     optimizer.step(closure=optimizer_closure)
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank11]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank11]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank11]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank11]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank11]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank11]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank11]:     return optimizer.step(closure=closure, **kwargs)
[rank11]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank11]:     out = func(*args, **kwargs)
[rank11]:           ^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank11]:     loss = closure()
[rank11]:            ^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank11]:     self._after_closure(model, optimizer)
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank11]:     self._clip_gradients(
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank11]:     call._call_lightning_module_hook(
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank11]:     output = fn(*args, **kwargs)
[rank11]:              ^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank11]:     self.clip_gradients(
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank11]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank11]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank11]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank11]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank11]:     return func(*args, **kwargs)
[rank11]:            ^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank11]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank11]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank11]:     return func(*args, **kwargs)
[rank11]:            ^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank11]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank11]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank11]:     return disable_fn(*args, **kwargs)
[rank11]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank11]:     return fn(*args, **kwargs)
[rank11]:            ^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank11]:     return DTensor._op_dispatcher.dispatch(
[rank11]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank11]:     self.sharding_propagator.propagate(op_info)
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank11]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank11]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank11]:     return self.cache(*args, **kwargs)
[rank11]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank11]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank11]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank11]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank11]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank11]:     raise ValueError(
[rank11]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63], mesh_dim_names=('data_parallel',)).
[rank63]: Traceback (most recent call last):
[rank63]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank63]:     main()
[rank63]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank63]:     llm.api.finetune(
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank63]:     return train(
[rank63]:            ^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank63]:     trainer.fit(model, data)
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank63]:     call._call_and_handle_interrupt(
[rank36]: Traceback (most recent call last):
[rank36]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank36]:     main()
[rank36]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank36]:     llm.api.finetune(
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank36]:     return train(
[rank36]:            ^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank36]:     trainer.fit(model, data)
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank36]:     call._call_and_handle_interrupt(
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank63]:     return trainer_fn(*args, **kwargs)
[rank63]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank63]:     self._run(model, ckpt_path=ckpt_path)
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank63]:     results = self._run_stage()
[rank63]:               ^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank63]:     self.fit_loop.run()
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank63]:     self.advance()
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank63]:     self.epoch_loop.run(self._data_fetcher)
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank63]:     self.advance(data_fetcher)
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank63]:     super().advance(data_fetcher)
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank36]:     return trainer_fn(*args, **kwargs)
[rank36]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank36]:     self._run(model, ckpt_path=ckpt_path)
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank36]:     results = self._run_stage()
[rank36]:               ^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank36]:     self.fit_loop.run()
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank63]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank63]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank63]:     self._optimizer_step(batch_idx, closure)
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank63]:     call._call_lightning_module_hook(
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank36]:     self.advance()
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank36]:     self.epoch_loop.run(self._data_fetcher)
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank36]:     self.advance(data_fetcher)
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank36]:     super().advance(data_fetcher)
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank63]:     output = fn(*args, **kwargs)
[rank63]:              ^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank36]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank36]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank36]:     self._optimizer_step(batch_idx, closure)
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank36]:     call._call_lightning_module_hook(
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank63]:     optimizer.step(closure=optimizer_closure)
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank63]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank63]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank63]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank63]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank36]:     output = fn(*args, **kwargs)
[rank36]:              ^^^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank63]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank63]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank63]:     return optimizer.step(closure=closure, **kwargs)
[rank63]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank63]:     out = func(*args, **kwargs)
[rank63]:           ^^^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank36]:     optimizer.step(closure=optimizer_closure)
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank36]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank36]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank36]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank36]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank63]:     loss = closure()
[rank63]:            ^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank63]:     self._after_closure(model, optimizer)
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank63]:     self._clip_gradients(
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank63]:     call._call_lightning_module_hook(
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank36]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank36]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank36]:     return optimizer.step(closure=closure, **kwargs)
[rank36]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank36]:     out = func(*args, **kwargs)
[rank36]:           ^^^^^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank63]:     output = fn(*args, **kwargs)
[rank63]:              ^^^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank63]:     self.clip_gradients(
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank63]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank36]:     loss = closure()
[rank36]:            ^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank36]:     self._after_closure(model, optimizer)
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank36]:     self._clip_gradients(
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank36]:     call._call_lightning_module_hook(
[rank63]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank36]:     output = fn(*args, **kwargs)
[rank36]:              ^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank36]:     self.clip_gradients(
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank36]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank61]: Traceback (most recent call last):
[rank61]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank61]:     main()
[rank61]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank61]:     llm.api.finetune(
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank61]:     return train(
[rank61]:            ^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank61]:     trainer.fit(model, data)
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank61]:     call._call_and_handle_interrupt(
[rank36]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank61]:     return trainer_fn(*args, **kwargs)
[rank61]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank61]:     self._run(model, ckpt_path=ckpt_path)
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank61]:     results = self._run_stage()
[rank61]:               ^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank61]:     self.fit_loop.run()
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank36]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank36]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank36]:     return func(*args, **kwargs)
[rank36]:            ^^^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank61]:     self.advance()
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank61]:     self.epoch_loop.run(self._data_fetcher)
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank61]:     self.advance(data_fetcher)
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank61]:     super().advance(data_fetcher)
[rank36]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank36]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank36]:     return func(*args, **kwargs)
[rank36]:            ^^^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank36]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank36]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank36]:     return disable_fn(*args, **kwargs)
[rank36]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank61]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank61]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank61]:     self._optimizer_step(batch_idx, closure)
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank61]:     call._call_lightning_module_hook(
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank36]:     return fn(*args, **kwargs)
[rank36]:            ^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank36]:     return DTensor._op_dispatcher.dispatch(
[rank36]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank36]:     self.sharding_propagator.propagate(op_info)
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank61]:     output = fn(*args, **kwargs)
[rank61]:              ^^^^^^^^^^^^^^^^^^^
[rank36]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank36]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank36]:     return self.cache(*args, **kwargs)
[rank36]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank36]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank36]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank63]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank63]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank63]:     return func(*args, **kwargs)
[rank63]:            ^^^^^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank36]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank36]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank63]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank63]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank63]:     return func(*args, **kwargs)
[rank63]:            ^^^^^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank63]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank63]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank63]:     return disable_fn(*args, **kwargs)
[rank63]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank36]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank36]:     raise ValueError(
[rank36]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60], mesh_dim_names=('data_parallel',)).
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank63]:     return fn(*args, **kwargs)
[rank63]:            ^^^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank63]:     return DTensor._op_dispatcher.dispatch(
[rank63]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank63]:     self.sharding_propagator.propagate(op_info)
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank63]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank63]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank63]:     return self.cache(*args, **kwargs)
[rank63]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank63]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank63]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank63]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank63]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank61]:     optimizer.step(closure=optimizer_closure)
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank61]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank61]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank61]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank61]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank61]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank61]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank61]:     return optimizer.step(closure=closure, **kwargs)
[rank61]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank61]:     out = func(*args, **kwargs)
[rank61]:           ^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank61]:     loss = closure()
[rank61]:            ^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank61]:     self._after_closure(model, optimizer)
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank61]:     self._clip_gradients(
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank61]:     call._call_lightning_module_hook(
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank61]:     output = fn(*args, **kwargs)
[rank61]:              ^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank61]:     self.clip_gradients(
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank61]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank61]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank63]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank63]:     raise ValueError(
[rank63]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63], mesh_dim_names=('data_parallel',)).
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank61]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank61]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank61]:     return func(*args, **kwargs)
[rank61]:            ^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank61]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank61]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank61]:     return func(*args, **kwargs)
[rank61]:            ^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank61]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank61]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank61]:     return disable_fn(*args, **kwargs)
[rank61]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank61]:     return fn(*args, **kwargs)
[rank61]:            ^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank61]:     return DTensor._op_dispatcher.dispatch(
[rank61]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank61]:     self.sharding_propagator.propagate(op_info)
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank38]: Traceback (most recent call last):
[rank38]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank38]:     main()
[rank38]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank38]:     llm.api.finetune(
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank38]:     return train(
[rank38]:            ^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank38]:     trainer.fit(model, data)
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank38]:     call._call_and_handle_interrupt(
[rank61]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank61]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank61]:     return self.cache(*args, **kwargs)
[rank61]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank61]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank61]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank38]:     return trainer_fn(*args, **kwargs)
[rank38]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank38]:     self._run(model, ckpt_path=ckpt_path)
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank38]:     results = self._run_stage()
[rank38]:               ^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank38]:     self.fit_loop.run()
[rank61]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank61]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank61]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank61]:     raise ValueError(
[rank61]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61], mesh_dim_names=('data_parallel',)).
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank38]:     self.advance()
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank38]:     self.epoch_loop.run(self._data_fetcher)
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank38]:     self.advance(data_fetcher)
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank38]:     super().advance(data_fetcher)
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank38]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank38]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank38]:     self._optimizer_step(batch_idx, closure)
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank38]:     call._call_lightning_module_hook(
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank38]:     output = fn(*args, **kwargs)
[rank38]:              ^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank38]:     optimizer.step(closure=optimizer_closure)
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank38]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank38]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank38]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank38]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank38]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank38]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank38]:     return optimizer.step(closure=closure, **kwargs)
[rank38]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank38]:     out = func(*args, **kwargs)
[rank38]:           ^^^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank38]:     loss = closure()
[rank38]:            ^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank38]:     self._after_closure(model, optimizer)
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank38]:     self._clip_gradients(
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank38]:     call._call_lightning_module_hook(
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank38]:     output = fn(*args, **kwargs)
[rank38]:              ^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank38]:     self.clip_gradients(
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank38]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank38]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank38]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank38]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank38]:     return func(*args, **kwargs)
[rank38]:            ^^^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank38]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank38]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank38]:     return func(*args, **kwargs)
[rank38]:            ^^^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank38]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank38]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank38]:     return disable_fn(*args, **kwargs)
[rank38]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank38]:     return fn(*args, **kwargs)
[rank38]:            ^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank38]:     return DTensor._op_dispatcher.dispatch(
[rank38]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank38]:     self.sharding_propagator.propagate(op_info)
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank38]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank38]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank38]:     return self.cache(*args, **kwargs)
[rank38]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank38]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank38]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank38]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank38]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank38]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank38]:     raise ValueError(
[rank38]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62], mesh_dim_names=('data_parallel',)).
[rank14]: Traceback (most recent call last):
[rank14]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank14]:     main()
[rank14]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank14]:     llm.api.finetune(
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank14]:     return train(
[rank14]:            ^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank14]:     trainer.fit(model, data)
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank14]:     call._call_and_handle_interrupt(
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank14]:     return trainer_fn(*args, **kwargs)
[rank14]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank14]:     self._run(model, ckpt_path=ckpt_path)
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank14]:     results = self._run_stage()
[rank14]:               ^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank14]:     self.fit_loop.run()
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank14]:     self.advance()
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank14]:     self.epoch_loop.run(self._data_fetcher)
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank14]:     self.advance(data_fetcher)
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank14]:     super().advance(data_fetcher)
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank14]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank14]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank14]:     self._optimizer_step(batch_idx, closure)
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank14]:     call._call_lightning_module_hook(
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank14]:     output = fn(*args, **kwargs)
[rank14]:              ^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank14]:     optimizer.step(closure=optimizer_closure)
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank14]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank14]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank14]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank14]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank60]: Traceback (most recent call last):
[rank60]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank60]:     main()
[rank60]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank60]:     llm.api.finetune(
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank60]:     return train(
[rank60]:            ^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank60]:     trainer.fit(model, data)
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank60]:     call._call_and_handle_interrupt(
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank14]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank14]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank14]:     return optimizer.step(closure=closure, **kwargs)
[rank14]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank14]:     out = func(*args, **kwargs)
[rank14]:           ^^^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank60]:     return trainer_fn(*args, **kwargs)
[rank60]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank60]:     self._run(model, ckpt_path=ckpt_path)
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank60]:     results = self._run_stage()
[rank60]:               ^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank60]:     self.fit_loop.run()
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank14]:     loss = closure()
[rank14]:            ^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank14]:     self._after_closure(model, optimizer)
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank14]:     self._clip_gradients(
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank14]:     call._call_lightning_module_hook(
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank60]:     self.advance()
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank60]:     self.epoch_loop.run(self._data_fetcher)
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank60]:     self.advance(data_fetcher)
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank60]:     super().advance(data_fetcher)
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank14]:     output = fn(*args, **kwargs)
[rank14]:              ^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank14]:     self.clip_gradients(
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank14]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank60]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank60]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank60]:     self._optimizer_step(batch_idx, closure)
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank60]:     call._call_lightning_module_hook(
[rank14]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank60]:     output = fn(*args, **kwargs)
[rank60]:              ^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank14]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank14]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank14]:     return func(*args, **kwargs)
[rank14]:            ^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank60]:     optimizer.step(closure=optimizer_closure)
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank60]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank60]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank60]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank60]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank14]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank14]:     return func(*args, **kwargs)
[rank14]:            ^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank14]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank14]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank14]:     return disable_fn(*args, **kwargs)
[rank14]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank60]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank60]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank60]:     return optimizer.step(closure=closure, **kwargs)
[rank60]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank60]:     out = func(*args, **kwargs)
[rank60]:           ^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank14]:     return fn(*args, **kwargs)
[rank14]:            ^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank14]:     return DTensor._op_dispatcher.dispatch(
[rank14]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank14]:     self.sharding_propagator.propagate(op_info)
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank60]:     loss = closure()
[rank60]:            ^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank60]:     self._after_closure(model, optimizer)
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank60]:     self._clip_gradients(
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank60]:     call._call_lightning_module_hook(
[rank14]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank14]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank14]:     return self.cache(*args, **kwargs)
[rank14]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank14]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank14]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank60]:     output = fn(*args, **kwargs)
[rank60]:              ^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank60]:     self.clip_gradients(
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank60]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank14]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank14]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank60]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank14]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank14]:     raise ValueError(
[rank14]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62], mesh_dim_names=('data_parallel',)).
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank60]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank60]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank60]:     return func(*args, **kwargs)
[rank60]:            ^^^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank60]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank60]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank60]:     return func(*args, **kwargs)
[rank60]:            ^^^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank60]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank60]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank60]:     return disable_fn(*args, **kwargs)
[rank60]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank60]:     return fn(*args, **kwargs)
[rank60]:            ^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank60]:     return DTensor._op_dispatcher.dispatch(
[rank60]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank60]:     self.sharding_propagator.propagate(op_info)
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank60]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank60]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank60]:     return self.cache(*args, **kwargs)
[rank60]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank60]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank60]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank60]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank60]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank60]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank60]:     raise ValueError(
[rank60]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60], mesh_dim_names=('data_parallel',)).
[rank20]: Traceback (most recent call last):
[rank20]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank20]:     main()
[rank20]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank20]:     llm.api.finetune(
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank20]:     return train(
[rank20]:            ^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank20]:     trainer.fit(model, data)
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank20]:     call._call_and_handle_interrupt(
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank20]:     return trainer_fn(*args, **kwargs)
[rank20]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank20]:     self._run(model, ckpt_path=ckpt_path)
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank20]:     results = self._run_stage()
[rank20]:               ^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank20]:     self.fit_loop.run()
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank20]:     self.advance()
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank20]:     self.epoch_loop.run(self._data_fetcher)
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank20]:     self.advance(data_fetcher)
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank20]:     super().advance(data_fetcher)
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank20]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank20]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank20]:     self._optimizer_step(batch_idx, closure)
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank20]:     call._call_lightning_module_hook(
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank20]:     output = fn(*args, **kwargs)
[rank20]:              ^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank20]:     optimizer.step(closure=optimizer_closure)
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank20]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank20]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank20]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank20]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank20]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank20]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank20]:     return optimizer.step(closure=closure, **kwargs)
[rank20]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank20]:     out = func(*args, **kwargs)
[rank20]:           ^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank20]:     loss = closure()
[rank20]:            ^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank20]:     self._after_closure(model, optimizer)
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank20]:     self._clip_gradients(
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank20]:     call._call_lightning_module_hook(
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank20]:     output = fn(*args, **kwargs)
[rank20]:              ^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank20]:     self.clip_gradients(
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank20]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank20]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank20]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank20]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank20]:     return func(*args, **kwargs)
[rank20]:            ^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank20]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank20]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank20]:     return func(*args, **kwargs)
[rank20]:            ^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank20]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank20]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank20]:     return disable_fn(*args, **kwargs)
[rank20]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank20]:     return fn(*args, **kwargs)
[rank20]:            ^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank20]:     return DTensor._op_dispatcher.dispatch(
[rank20]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank20]:     self.sharding_propagator.propagate(op_info)
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank20]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank20]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank20]:     return self.cache(*args, **kwargs)
[rank20]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank20]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank20]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank20]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank20]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank20]:     raise ValueError(
[rank20]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60], mesh_dim_names=('data_parallel',)).
[rank62]: Traceback (most recent call last):
[rank62]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank62]:     main()
[rank62]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank62]:     llm.api.finetune(
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank62]:     return train(
[rank62]:            ^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank62]:     trainer.fit(model, data)
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank62]:     call._call_and_handle_interrupt(
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank62]:     return trainer_fn(*args, **kwargs)
[rank62]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank62]:     self._run(model, ckpt_path=ckpt_path)
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank62]:     results = self._run_stage()
[rank62]:               ^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank62]:     self.fit_loop.run()
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank62]:     self.advance()
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank62]:     self.epoch_loop.run(self._data_fetcher)
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank62]:     self.advance(data_fetcher)
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank62]:     super().advance(data_fetcher)
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank62]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank62]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank62]:     self._optimizer_step(batch_idx, closure)
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank62]:     call._call_lightning_module_hook(
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank62]:     output = fn(*args, **kwargs)
[rank62]:              ^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank62]:     optimizer.step(closure=optimizer_closure)
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank62]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank62]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank62]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank62]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank62]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank62]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank62]:     return optimizer.step(closure=closure, **kwargs)
[rank62]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank62]:     out = func(*args, **kwargs)
[rank62]:           ^^^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank62]:     loss = closure()
[rank62]:            ^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank62]:     self._after_closure(model, optimizer)
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank62]:     self._clip_gradients(
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank62]:     call._call_lightning_module_hook(
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank62]:     output = fn(*args, **kwargs)
[rank62]:              ^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank62]:     self.clip_gradients(
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank62]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank62]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank62]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank62]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank62]:     return func(*args, **kwargs)
[rank62]:            ^^^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank62]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank62]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank62]:     return func(*args, **kwargs)
[rank62]:            ^^^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank62]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank62]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank62]:     return disable_fn(*args, **kwargs)
[rank62]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank62]:     return fn(*args, **kwargs)
[rank62]:            ^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank62]:     return DTensor._op_dispatcher.dispatch(
[rank62]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank62]:     self.sharding_propagator.propagate(op_info)
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank62]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank62]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank62]:     return self.cache(*args, **kwargs)
[rank62]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank62]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank62]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank62]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank62]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]: Traceback (most recent call last):
[rank13]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank13]:     main()
[rank13]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank13]:     llm.api.finetune(
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank13]:     return train(
[rank13]:            ^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank13]:     trainer.fit(model, data)
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank13]:     call._call_and_handle_interrupt(
[rank62]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank62]:     raise ValueError(
[rank62]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62], mesh_dim_names=('data_parallel',)).
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank13]:     return trainer_fn(*args, **kwargs)
[rank13]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank13]:     self._run(model, ckpt_path=ckpt_path)
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank13]:     results = self._run_stage()
[rank13]:               ^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank13]:     self.fit_loop.run()
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank13]:     self.advance()
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank13]:     self.epoch_loop.run(self._data_fetcher)
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank13]:     self.advance(data_fetcher)
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank13]:     super().advance(data_fetcher)
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank13]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank13]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank13]:     self._optimizer_step(batch_idx, closure)
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank13]:     call._call_lightning_module_hook(
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank13]:     output = fn(*args, **kwargs)
[rank13]:              ^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank13]:     optimizer.step(closure=optimizer_closure)
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank13]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank13]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank13]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank13]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank13]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank13]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank13]:     return optimizer.step(closure=closure, **kwargs)
[rank13]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank13]:     out = func(*args, **kwargs)
[rank13]:           ^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank13]:     loss = closure()
[rank13]:            ^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank13]:     self._after_closure(model, optimizer)
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank13]:     self._clip_gradients(
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank13]:     call._call_lightning_module_hook(
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank13]:     output = fn(*args, **kwargs)
[rank13]:              ^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank13]:     self.clip_gradients(
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank13]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank13]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank15]: Traceback (most recent call last):
[rank15]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank15]:     main()
[rank15]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank15]:     llm.api.finetune(
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank15]:     return train(
[rank15]:            ^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank15]:     trainer.fit(model, data)
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank15]:     call._call_and_handle_interrupt(
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank15]:     return trainer_fn(*args, **kwargs)
[rank15]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank15]:     self._run(model, ckpt_path=ckpt_path)
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank15]:     results = self._run_stage()
[rank15]:               ^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank15]:     self.fit_loop.run()
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank15]:     self.advance()
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank15]:     self.epoch_loop.run(self._data_fetcher)
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank15]:     self.advance(data_fetcher)
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank15]:     super().advance(data_fetcher)
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank15]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank15]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank15]:     self._optimizer_step(batch_idx, closure)
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank15]:     call._call_lightning_module_hook(
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank15]:     output = fn(*args, **kwargs)
[rank15]:              ^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank13]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank13]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank13]:     return func(*args, **kwargs)
[rank13]:            ^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank13]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank13]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank13]:     return func(*args, **kwargs)
[rank13]:            ^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank13]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank13]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank13]:     return disable_fn(*args, **kwargs)
[rank13]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank13]:     return fn(*args, **kwargs)
[rank13]:            ^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank13]:     return DTensor._op_dispatcher.dispatch(
[rank13]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank13]:     self.sharding_propagator.propagate(op_info)
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank13]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank13]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank13]:     return self.cache(*args, **kwargs)
[rank13]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank13]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank13]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank13]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank13]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank15]:     optimizer.step(closure=optimizer_closure)
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank15]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank15]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank15]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank15]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank15]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank15]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank15]:     return optimizer.step(closure=closure, **kwargs)
[rank15]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank15]:     out = func(*args, **kwargs)
[rank15]:           ^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank15]:     loss = closure()
[rank15]:            ^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank15]:     self._after_closure(model, optimizer)
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank15]:     self._clip_gradients(
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank15]:     call._call_lightning_module_hook(
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank15]:     output = fn(*args, **kwargs)
[rank15]:              ^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank15]:     self.clip_gradients(
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank15]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank15]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank13]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank13]:     raise ValueError(
[rank13]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61], mesh_dim_names=('data_parallel',)).
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank15]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank15]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank15]:     return func(*args, **kwargs)
[rank15]:            ^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank15]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank15]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank15]:     return func(*args, **kwargs)
[rank15]:            ^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank15]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank15]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank15]:     return disable_fn(*args, **kwargs)
[rank15]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank15]:     return fn(*args, **kwargs)
[rank15]:            ^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank15]:     return DTensor._op_dispatcher.dispatch(
[rank15]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank15]:     self.sharding_propagator.propagate(op_info)
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank15]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank15]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank15]:     return self.cache(*args, **kwargs)
[rank15]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank15]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank15]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank15]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank15]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank15]:     raise ValueError(
[rank15]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63], mesh_dim_names=('data_parallel',)).
[rank12]: Traceback (most recent call last):
[rank12]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank12]:     main()
[rank12]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank12]:     llm.api.finetune(
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank12]:     return train(
[rank12]:            ^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank12]:     trainer.fit(model, data)
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank12]:     call._call_and_handle_interrupt(
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank12]:     return trainer_fn(*args, **kwargs)
[rank12]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank12]:     self._run(model, ckpt_path=ckpt_path)
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank12]:     results = self._run_stage()
[rank12]:               ^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank12]:     self.fit_loop.run()
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank12]:     self.advance()
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank12]:     self.epoch_loop.run(self._data_fetcher)
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank12]:     self.advance(data_fetcher)
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank12]:     super().advance(data_fetcher)
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank12]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank12]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank12]:     self._optimizer_step(batch_idx, closure)
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank12]:     call._call_lightning_module_hook(
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank12]:     output = fn(*args, **kwargs)
[rank12]:              ^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank12]:     optimizer.step(closure=optimizer_closure)
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank12]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank12]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank12]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank12]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank12]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank12]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank12]:     return optimizer.step(closure=closure, **kwargs)
[rank12]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank12]:     out = func(*args, **kwargs)
[rank12]:           ^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank12]:     loss = closure()
[rank12]:            ^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank12]:     self._after_closure(model, optimizer)
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank12]:     self._clip_gradients(
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank12]:     call._call_lightning_module_hook(
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank12]:     output = fn(*args, **kwargs)
[rank12]:              ^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank12]:     self.clip_gradients(
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank12]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank12]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank12]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank12]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank12]:     return func(*args, **kwargs)
[rank12]:            ^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank12]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank12]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank12]:     return func(*args, **kwargs)
[rank12]:            ^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank12]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank12]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank12]:     return disable_fn(*args, **kwargs)
[rank12]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank12]:     return fn(*args, **kwargs)
[rank12]:            ^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank12]:     return DTensor._op_dispatcher.dispatch(
[rank12]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank12]:     self.sharding_propagator.propagate(op_info)
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank12]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank12]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank12]:     return self.cache(*args, **kwargs)
[rank12]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank12]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank12]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank12]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank12]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank12]:     raise ValueError(
[rank12]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60], mesh_dim_names=('data_parallel',)).
[rank23]: Traceback (most recent call last):
[rank23]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank23]:     main()
[rank23]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank23]:     llm.api.finetune(
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank23]:     return train(
[rank23]:            ^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank23]:     trainer.fit(model, data)
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank23]:     call._call_and_handle_interrupt(
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank23]:     return trainer_fn(*args, **kwargs)
[rank23]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank23]:     self._run(model, ckpt_path=ckpt_path)
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank23]:     results = self._run_stage()
[rank23]:               ^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank23]:     self.fit_loop.run()
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank23]:     self.advance()
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank23]:     self.epoch_loop.run(self._data_fetcher)
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank23]:     self.advance(data_fetcher)
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank23]:     super().advance(data_fetcher)
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank23]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank23]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank23]:     self._optimizer_step(batch_idx, closure)
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank23]:     call._call_lightning_module_hook(
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank23]:     output = fn(*args, **kwargs)
[rank23]:              ^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank23]:     optimizer.step(closure=optimizer_closure)
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank23]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank23]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank23]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank23]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank23]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank23]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank23]:     return optimizer.step(closure=closure, **kwargs)
[rank23]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank23]:     out = func(*args, **kwargs)
[rank23]:           ^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank23]:     loss = closure()
[rank23]:            ^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank23]:     self._after_closure(model, optimizer)
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank23]:     self._clip_gradients(
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank23]:     call._call_lightning_module_hook(
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank23]:     output = fn(*args, **kwargs)
[rank23]:              ^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank23]:     self.clip_gradients(
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank23]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank23]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank23]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank23]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank23]:     return func(*args, **kwargs)
[rank23]:            ^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank23]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank23]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank23]:     return func(*args, **kwargs)
[rank23]:            ^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank23]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank23]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank23]:     return disable_fn(*args, **kwargs)
[rank23]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank23]:     return fn(*args, **kwargs)
[rank23]:            ^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank23]:     return DTensor._op_dispatcher.dispatch(
[rank23]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank23]:     self.sharding_propagator.propagate(op_info)
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank23]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank23]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank23]:     return self.cache(*args, **kwargs)
[rank23]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank23]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank23]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank23]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank23]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank23]:     raise ValueError(
[rank23]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63], mesh_dim_names=('data_parallel',)).
[rank21]: Traceback (most recent call last):
[rank21]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank21]:     main()
[rank21]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank21]:     llm.api.finetune(
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank21]:     return train(
[rank21]:            ^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank21]:     trainer.fit(model, data)
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank21]:     call._call_and_handle_interrupt(
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank21]:     return trainer_fn(*args, **kwargs)
[rank21]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank21]:     self._run(model, ckpt_path=ckpt_path)
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank21]:     results = self._run_stage()
[rank21]:               ^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank21]:     self.fit_loop.run()
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank21]:     self.advance()
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank21]:     self.epoch_loop.run(self._data_fetcher)
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank21]:     self.advance(data_fetcher)
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank21]:     super().advance(data_fetcher)
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank21]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank21]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank21]:     self._optimizer_step(batch_idx, closure)
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank21]:     call._call_lightning_module_hook(
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank21]:     output = fn(*args, **kwargs)
[rank21]:              ^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank21]:     optimizer.step(closure=optimizer_closure)
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank21]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank21]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank21]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank21]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank21]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank21]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank21]:     return optimizer.step(closure=closure, **kwargs)
[rank21]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank21]:     out = func(*args, **kwargs)
[rank21]:           ^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank21]:     loss = closure()
[rank21]:            ^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank21]:     self._after_closure(model, optimizer)
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank21]:     self._clip_gradients(
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank21]:     call._call_lightning_module_hook(
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank21]:     output = fn(*args, **kwargs)
[rank21]:              ^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank21]:     self.clip_gradients(
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank21]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank21]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank21]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank21]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank21]:     return func(*args, **kwargs)
[rank21]:            ^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank21]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank21]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank21]:     return func(*args, **kwargs)
[rank21]:            ^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank21]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank21]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank21]:     return disable_fn(*args, **kwargs)
[rank21]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank21]:     return fn(*args, **kwargs)
[rank21]:            ^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank21]:     return DTensor._op_dispatcher.dispatch(
[rank21]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank21]:     self.sharding_propagator.propagate(op_info)
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank21]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank21]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank21]:     return self.cache(*args, **kwargs)
[rank21]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank21]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank21]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank21]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank21]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank21]:     raise ValueError(
[rank21]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61], mesh_dim_names=('data_parallel',)).
[rank22]: Traceback (most recent call last):
[rank22]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 524, in <module>
[rank22]:     main()
[rank22]:   File "/lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_TPFSDP2.py", line 494, in main
[rank22]:     llm.api.finetune(
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 236, in finetune
[rank22]:     return train(
[rank22]:            ^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/collections/llm/api.py", line 135, in train
[rank22]:     trainer.fit(model, data)
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank22]:     call._call_and_handle_interrupt(
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank22]:     return trainer_fn(*args, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank22]:     self._run(model, ckpt_path=ckpt_path)
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank22]:     results = self._run_stage()
[rank22]:               ^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank22]:     self.fit_loop.run()
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank22]:     self.advance()
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank22]:     self.epoch_loop.run(self._data_fetcher)
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank22]:     self.advance(data_fetcher)
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/nemo/lightning/pytorch/trainer.py", line 47, in advance
[rank22]:     super().advance(data_fetcher)
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
[rank22]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank22]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
[rank22]:     self._optimizer_step(batch_idx, closure)
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank22]:     call._call_lightning_module_hook(
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank22]:     output = fn(*args, **kwargs)
[rank22]:              ^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
[rank22]:     optimizer.step(closure=optimizer_closure)
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 153, in step
[rank22]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank22]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
[rank22]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 75, in optimizer_step
[rank22]:     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
[rank22]:     return optimizer.step(closure=closure, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank22]:     out = func(*args, **kwargs)
[rank22]:           ^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/transformer_engine/pytorch/optimizers/fused_adam.py", line 486, in step
[rank22]:     loss = closure()
[rank22]:            ^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank22]:     self._after_closure(model, optimizer)
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 88, in _after_closure
[rank22]:     self._clip_gradients(
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 135, in _clip_gradients
[rank22]:     call._call_lightning_module_hook(
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
[rank22]:     output = fn(*args, **kwargs)
[rank22]:              ^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1243, in configure_gradient_clipping
[rank22]:     self.clip_gradients(
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1214, in clip_gradients
[rank22]:     self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py", line 111, in clip_gradients
[rank22]:     super().clip_gradients(optimizer=optimizer, clip_val=clip_val, gradient_clip_algorithm=gradient_clip_algorithm)
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 155, in clip_gradients
[rank22]:     self.clip_grad_by_norm(optimizer, clip_val)
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py", line 165, in clip_grad_by_norm
[rank22]:     torch.nn.utils.clip_grad_norm_(parameters, clip_val)
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank22]:     return func(*args, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 221, in clip_grad_norm_
[rank22]:     total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)
[rank22]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 36, in _no_grad_wrapper
[rank22]:     return func(*args, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 100, in _get_total_norm
[rank22]:     torch.stack([norm.to(first_device) for norm in norms]), norm_type
[rank22]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
[rank22]:     return disable_fn(*args, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
[rank22]:     return fn(*args, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_api.py", line 350, in __torch_dispatch__
[rank22]:     return DTensor._op_dispatcher.dispatch(
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py", line 154, in dispatch
[rank22]:     self.sharding_propagator.propagate(op_info)
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 266, in propagate
[rank22]:     OutputSharding, self.propagate_op_sharding(op_info.schema)
[rank22]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 45, in __call__
[rank22]:     return self.cache(*args, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_sharding_prop.py", line 286, in propagate_op_sharding_non_cached
[rank22]:     op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
[rank22]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 641, in stack_strategy
[rank22]:     follow_placements = _derive_follow_placements_from_tuple_strategy(
[rank22]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/lustre/fshomisc/sup/pub/miniforge/24.11.3/envs/nemo-2.4.0+py3.12.10/lib/python3.12/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py", line 593, in _derive_follow_placements_from_tuple_strategy
[rank22]:     raise ValueError(
[rank22]: ValueError: All operands in aten.stack.default must have the same mesh, but got DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('data_parallel', 'tensor_parallel')) and DeviceMesh('cuda', [2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62], mesh_dim_names=('data_parallel',)).
srun: error: jzxh161: task 47: Exited with exit code 1
srun: Terminating StepId=1542780.0
slurmstepd: error: *** STEP 1542780.0 ON jzxh003 CANCELLED AT 2025-11-09T13:47:05 ***
srun: error: jzxh337: task 52: Exited with exit code 1
srun: error: jzxh357: task 63: Exited with exit code 1
srun: error: jzxh356: tasks 57,59: Exited with exit code 1
srun: error: jzxh160: tasks 40-41: Exited with exit code 1
srun: error: jzxh092: task 8: Exited with exit code 1
srun: error: jzxh122: tasks 12,15: Exited with exit code 1
srun: error: jzxh091: tasks 4,6: Exited with exit code 1
srun: error: jzxh140: tasks 32-34: Exited with exit code 1
srun: error: jzxh161: tasks 44-45: Exited with exit code 1
srun: error: jzxh337: tasks 54-55: Exited with exit code 1
srun: error: jzxh159: tasks 36,38-39: Exited with exit code 1
srun: error: jzxh357: tasks 61-62: Exited with exit code 1
srun: error: jzxh003: tasks 0-2: Exited with exit code 1
srun: error: jzxh123: tasks 16-18: Exited with exit code 1
srun: error: jzxh336: tasks 48,50-51: Exited with exit code 1
srun: error: jzxh125: tasks 20-21,23: Exited with exit code 1
srun: error: jzxh126: tasks 24-26: Exited with exit code 1
srun: error: jzxh122: task 14: Exited with exit code 1
srun: error: jzxh139: tasks 28-30: Exited with exit code 1
srun: error: jzxh356: task 58: Exited with exit code 1
srun: error: jzxh160: task 42: Exited with exit code 1
srun: error: jzxh092: tasks 9-10: Exited with exit code 1
srun: error: jzxh091: task 7: Exited with exit code 1
srun: error: jzxh161: task 46: Exited with exit code 1
srun: error: jzxh337: task 53: Exited with exit code 1
srun: error: jzxh160: task 43: Exited with exit code 1
srun: error: jzxh356: task 56: Exited with exit code 1
srun: error: jzxh357: task 60: Exited with exit code 1
srun: error: jzxh122: task 13: Exited with exit code 1
srun: error: jzxh140: task 35: Exited with exit code 1
srun: error: jzxh092: task 11: Exited with exit code 1
srun: error: jzxh125: task 22: Exited with exit code 1
srun: error: jzxh123: task 19: Exited with exit code 1
srun: error: jzxh126: task 27: Exited with exit code 1
srun: error: jzxh159: task 37: Exited with exit code 1
srun: error: jzxh003: task 3: Exited with exit code 1
srun: error: jzxh091: task 5: Exited with exit code 1
srun: error: jzxh336: task 49: Exited with exit code 1
srun: error: jzxh139: task 31: Exited with exit code 1

real	1m33.442s
user	0m0.029s
sys	0m0.048s
+ date
